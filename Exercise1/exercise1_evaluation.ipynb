{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "638e2d14",
   "metadata": {
    "id": "638e2d14"
   },
   "source": [
    "# Exercise 1: MX Quantization of Linear Layers\n",
    "\n",
    "## Llama-3.2-1B with mxfp4_e2m1 (weights) + mxfp6_e2m3 (activations)\n",
    "\n",
    "This notebook evaluates an MX-quantized Llama model on the `lambada_openai` task.\n",
    "\n",
    "**Exercise objectives**\n",
    "- Quantize all linear layers (Q, K, V, O, gate, up, down)\n",
    "- Use `mxfp4_e2m1` for weights (4-bit)\n",
    "- Use `mxfp6_e2m3` for activations (6-bit)\n",
    "- Compare accuracy vs baseline (62.10%)\n",
    "\n",
    "**Expected outcome**\n",
    "- Accuracy target: > 60% (< 2% degradation)\n",
    "\n",
    "**Author:** Pavan Chauhan  \n",
    "**Date:** January 30, 2026\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is designed to run top-to-bottom without manual restarts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270619e7",
   "metadata": {
    "id": "270619e7"
   },
   "source": [
    "## Step 1: Verify GPU Runtime\n",
    "\n",
    "Ensure GPU runtime is enabled (T4/A100/H100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4b91c23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4b91c23",
    "outputId": "bc29570a-093c-43a2-8a7a-9c19092881fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan 31 17:49:41 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   30C    P0             44W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488c0506",
   "metadata": {
    "id": "488c0506"
   },
   "source": [
    "## Step 2: Clone Project Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c04e39c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0c04e39c",
    "outputId": "bda364e2-fb74-4e2b-ff09-db999671afce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/msr-intern-project\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clone / update the project repository\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "repo_dir = \"/content/msr-intern-project\"\n",
    "if not os.path.exists(repo_dir):\n",
    "    subprocess.check_call([\"git\", \"clone\", \"https://github.com/pavannn16/msr-intern-project.git\", repo_dir])\n",
    "else:\n",
    "    subprocess.check_call([\"git\", \"-C\", repo_dir, \"fetch\", \"origin\", \"main\"])\n",
    "    subprocess.check_call([\"git\", \"-C\", repo_dir, \"reset\", \"--hard\", \"origin/main\"])\n",
    "\n",
    "%cd /content/msr-intern-project\n",
    "subprocess.check_call([\"git\", \"rev-parse\", \"--short\", \"HEAD\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c00750b",
   "metadata": {
    "id": "8c00750b"
   },
   "source": [
    "## Step 3: Install Dependencies\n",
    "\n",
    "Install `transformers`, `microxcaling` (MX), and `lm-eval`.\n",
    "\n",
    "Estimated time: 2-3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d67f3c57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d67f3c57",
    "outputId": "c65a55cc-2310-4d34-c793-11c6068269a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies...\n",
      "+ /usr/bin/python3 -m pip install -q transformers==4.57.6 lm_eval ninja\n",
      "torch=2.9.0+cu126, cuda=12.6, device_count=1\n",
      "microxcaling already exists\n",
      "+ /usr/bin/python3 -m pip install -q -e /content/microxcaling --no-deps\n",
      "MX import: OK\n",
      "All dependencies installed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"Installing dependencies...\")\n",
    "\n",
    "def pip_install(args):\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + args\n",
    "    print(\"+\", \" \".join(cmd))\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "# Core packages (do not force-install torch; Colab typically provides a working CUDA build)\n",
    "pip_install([\"transformers==4.57.6\", \"lm_eval\", \"ninja\"])\n",
    "\n",
    "# Confirm torch is available and report version\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"torch={torch.__version__}, cuda={torch.version.cuda}, device_count={torch.cuda.device_count()}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"PyTorch is not available in this runtime: {e}\")\n",
    "\n",
    "# Clone microxcaling (MX)\n",
    "mx_repo_dir = \"/content/microxcaling\"\n",
    "if not os.path.exists(mx_repo_dir):\n",
    "    subprocess.check_call([\"git\", \"clone\", \"-q\", \"https://github.com/microsoft/microxcaling.git\", mx_repo_dir])\n",
    "    print(\"microxcaling cloned\")\n",
    "else:\n",
    "    print(\"microxcaling already exists\")\n",
    "\n",
    "# Install microxcaling WITHOUT deps to avoid torchaudio pinning issues.\n",
    "# The Python package is commonly located under a subdirectory (e.g., /python).\n",
    "mx_python_root = os.path.join(mx_repo_dir, \"python\") if os.path.isdir(os.path.join(mx_repo_dir, \"python\")) else mx_repo_dir\n",
    "pip_install([\"-e\", mx_python_root, \"--no-deps\"])\n",
    "\n",
    "# Sanity check: MX import (fallback to sys.path if editable install layout differs)\n",
    "try:\n",
    "    import mx  # noqa: F401\n",
    "except ModuleNotFoundError:\n",
    "    if mx_python_root not in sys.path:\n",
    "        sys.path.insert(0, mx_python_root)\n",
    "    import mx  # noqa: F401\n",
    "\n",
    "print(\"MX import: OK\")\n",
    "print(\"All dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f3f3ef",
   "metadata": {
    "id": "64f3f3ef"
   },
   "source": [
    "## Step 4: Setup Exercise 1 Files\n",
    "\n",
    "Copy the complete MX-integrated modeling_llama.py to transformers package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47501700",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47501700",
    "outputId": "a11967fa-e715-414b-9138-e94be4d4f9e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Exercise 1 files...\n",
      "Transformers installed at: /usr/local/lib/python3.12/dist-packages/transformers\n",
      "Found MX-integrated file (725 lines)\n",
      "MX-integrated modeling_llama.py deployed\n",
      "MX quantization functions detected in deployed file\n",
      "\n",
      "Exercise 1 setup complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import transformers\n",
    "\n",
    "print(\"Setting up Exercise 1 files...\")\n",
    "\n",
    "transformers_path = os.path.dirname(transformers.__file__)\n",
    "print(f\"Transformers installed at: {transformers_path}\")\n",
    "\n",
    "modeling_llama_path = os.path.join(transformers_path, \"models\", \"llama\", \"modeling_llama.py\")\n",
    "backup_path = modeling_llama_path + \".backup\"\n",
    "\n",
    "mx_complete_file = \"/content/msr-intern-project/Exercise1/modified_files/modeling_llama.py\"\n",
    "if os.path.exists(mx_complete_file):\n",
    "    with open(mx_complete_file, \"r\") as f:\n",
    "        line_count = len(f.readlines())\n",
    "    print(f\"Found MX-integrated file ({line_count} lines)\")\n",
    "\n",
    "    if not os.path.exists(backup_path):\n",
    "        shutil.copy2(modeling_llama_path, backup_path)\n",
    "        print(\"Backup created\")\n",
    "\n",
    "    shutil.copy2(mx_complete_file, modeling_llama_path)\n",
    "    print(\"MX-integrated modeling_llama.py deployed\")\n",
    "\n",
    "    with open(modeling_llama_path, \"r\") as f:\n",
    "        content = f.read()\n",
    "    if \"apply_mx_linear\" in content:\n",
    "        print(\"MX quantization functions detected in deployed file\")\n",
    "    else:\n",
    "        print(\"Warning: MX functions not found in deployed file\")\n",
    "else:\n",
    "    print(f\"Warning: MX file not found at {mx_complete_file}\")\n",
    "    print(\"  Evaluation will use standard transformers (no MX quantization)\")\n",
    "\n",
    "print(\"\\nExercise 1 setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8839dd3a",
   "metadata": {
    "id": "8839dd3a"
   },
   "source": [
    "## Step 5: Set Hugging Face Token\n",
    "\n",
    "Set your Hugging Face token for model access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9b2fa9",
   "metadata": {
    "id": "af9b2fa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF token is set (value hidden)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Requirement: do NOT hardcode tokens in the notebook.\n",
    "# Set HF_TOKEN externally (e.g., Colab Secrets or `export HF_TOKEN=...` in the runtime).\n",
    "\n",
    "hf_token = os.environ.get(\"HF_TOKEN\") or os.environ.get(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "if not hf_token:\n",
    "    raise RuntimeError(\n",
    "        \"HF_TOKEN is not set. Set it via Colab Secrets (recommended) or export it in the runtime before running this cell.\"\n",
    "    )\n",
    "\n",
    "# Mirror to HF_TOKEN in case the user provided HUGGINGFACE_HUB_TOKEN.\n",
    "os.environ[\"HF_TOKEN\"] = hf_token\n",
    "\n",
    "print(\"HF token is set (value hidden)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56d8ed7",
   "metadata": {
    "id": "b56d8ed7"
   },
   "source": [
    "## Step 6: Verify MX Integration\n",
    "\n",
    "Test that MX library and modified model load correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2002d63b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "2002d63b",
    "outputId": "ac3da440-bdf8-47a1-9595-55ef4a90d62d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying MX integration...\n",
      "MX library import: OK\n",
      "MX Quantization Configuration:\n",
      "==================================================\n",
      "Weights: fp4_e2m1 (4-bit)\n",
      "Activations: fp6_e2m3 (6-bit)\n",
      "Scale Bits: 8 (E8M0)\n",
      "Block Size: 32\n",
      "CUDA Backend: Enabled\n",
      "Rounding: even\n",
      "Backward Quantization: Disabled\n",
      "==================================================\n",
      "Transformers llama module: /usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\n",
      "Deployed MX rounding mode: even\n",
      "Trial 2 semantics detected: True\n",
      "MX integration detected in LlamaMLP.forward.\n",
      "MX integration verification complete\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import inspect\n",
    "import sys\n",
    "\n",
    "print(\"Verifying MX integration...\")\n",
    "\n",
    "exercise1_dir = \"/content/msr-intern-project/Exercise1\"\n",
    "if exercise1_dir not in sys.path:\n",
    "    sys.path.insert(0, exercise1_dir)\n",
    "\n",
    "# 1) MX import\n",
    "try:\n",
    "    from mx import linear as mx_linear  # noqa: F401\n",
    "    from mx.specs import MxSpecs  # noqa: F401\n",
    "    print(\"MX library import: OK\")\n",
    "except ImportError as e:\n",
    "    raise ImportError(\n",
    "        f\"MX library import failed: {e}. Ensure Step 3 installed microxcaling (pip install -e /content/microxcaling).\"\n",
    "    )\n",
    "\n",
    "# 2) Local helper import\n",
    "try:\n",
    "    from mx_config_helper import create_mx_specs_exercise1, print_mx_specs_summary\n",
    "    mx_specs = create_mx_specs_exercise1()\n",
    "    print_mx_specs_summary(mx_specs)\n",
    "except ImportError as e:\n",
    "    raise ImportError(\n",
    "        f\"Failed to import Exercise 1 helper(s): {e}. Ensure the repo is cloned and Step 2 ran successfully.\"\n",
    "    )\n",
    "\n",
    "# 3) Confirm the deployed transformers Llama file is the MX-integrated one\n",
    "import transformers\n",
    "\n",
    "llama_mod = transformers.models.llama.modeling_llama\n",
    "llama_file = getattr(llama_mod, \"__file__\", \"<unknown>\")\n",
    "print(f\"Transformers llama module: {llama_file}\")\n",
    "\n",
    "with open(llama_file, \"r\") as f:\n",
    "    deployed_src = f.read()\n",
    "\n",
    "if \"apply_mx_linear\" not in deployed_src:\n",
    "    raise RuntimeError(\n",
    "        \"Deployed transformers modeling_llama.py does not appear to include MX integration. Re-run Step 4 (deploy).\"\n",
    "    )\n",
    "\n",
    "# Extra: confirm the deployed MX specs match expected settings (especially rounding).\n",
    "deployed_specs = getattr(llama_mod, \"EXERCISE1_MX_SPECS\", None)\n",
    "if deployed_specs is not None:\n",
    "    try:\n",
    "        deployed_round = deployed_specs.get(\"round\") if hasattr(deployed_specs, \"get\") else deployed_specs[\"round\"]\n",
    "    except Exception:\n",
    "        deployed_round = \"<unknown>\"\n",
    "    print(f\"Deployed MX rounding mode: {deployed_round}\")\n",
    "\n",
    "def _trial2_semantics_present(src):\n",
    "    \"\"\"Returns (ok, reason). ok=True means bias and mx_specs are passed into the MX linear call.\"\"\"\n",
    "    try:\n",
    "        tree = ast.parse(src)\n",
    "    except SyntaxError as e:\n",
    "        return False, f\"Unable to parse deployed modeling_llama.py: {e}\"\n",
    "\n",
    "    apply_fn = None\n",
    "    for node in tree.body:\n",
    "        if isinstance(node, ast.FunctionDef) and node.name == \"apply_mx_linear\":\n",
    "            apply_fn = node\n",
    "            break\n",
    "    if apply_fn is None:\n",
    "        return False, \"apply_mx_linear not found\"\n",
    "\n",
    "    # Guard against Trial-1-style manual bias add after MX op.\n",
    "    for node in ast.walk(apply_fn):\n",
    "        if isinstance(node, ast.BinOp) and isinstance(node.op, ast.Add):\n",
    "            left_is_bias = isinstance(node.left, ast.Name) and node.left.id == \"bias\"\n",
    "            right_is_bias = isinstance(node.right, ast.Name) and node.right.id == \"bias\"\n",
    "            if left_is_bias or right_is_bias:\n",
    "                return False, \"Found a manual '+ bias' in apply_mx_linear (likely Trial 1)\"\n",
    "\n",
    "    # Accept both patterns used across iterations/versions:\n",
    "    # - mx.linear(..., bias=bias, mx_specs=mx_specs)\n",
    "    # - mx_linear.linear(..., bias=bias, mx_specs=mx_specs)\n",
    "    # - mx_fn(..., bias=bias, mx_specs=mx_specs) where mx_fn = getattr(mx_linear, 'linear', mx_linear)\n",
    "    def call_passes_bias_and_specs(call):\n",
    "        bias_ok = False\n",
    "        specs_ok = False\n",
    "\n",
    "        # Positional forms: (..., bias, mx_specs)\n",
    "        if len(call.args) >= 3 and isinstance(call.args[2], ast.Name) and call.args[2].id == \"bias\":\n",
    "            bias_ok = True\n",
    "        if len(call.args) >= 4 and isinstance(call.args[3], ast.Name) and call.args[3].id == \"mx_specs\":\n",
    "            specs_ok = True\n",
    "\n",
    "        # Keyword forms: bias=bias, mx_specs=mx_specs\n",
    "        for kw in call.keywords or []:\n",
    "            if kw.arg == \"bias\" and isinstance(kw.value, ast.Name) and kw.value.id == \"bias\":\n",
    "                bias_ok = True\n",
    "            if kw.arg in {\"mx_specs\", \"specs\"} and isinstance(kw.value, ast.Name) and kw.value.id == \"mx_specs\":\n",
    "                specs_ok = True\n",
    "\n",
    "        return bias_ok and specs_ok\n",
    "\n",
    "    for node in ast.walk(apply_fn):\n",
    "        if isinstance(node, ast.Call) and call_passes_bias_and_specs(node):\n",
    "            return True, \"\"\n",
    "\n",
    "    return False, \"No call found in apply_mx_linear that passes bias and mx_specs\"\n",
    "\n",
    "trial2_ok, trial2_reason = _trial2_semantics_present(deployed_src)\n",
    "print(f\"Trial 2 semantics detected: {trial2_ok}\")\n",
    "if not trial2_ok:\n",
    "    print(f\"Trial 2 semantics check detail: {trial2_reason}\")\n",
    "\n",
    "# 4) Smoke import of model classes\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM, LlamaMLP, LlamaAttention  # noqa: F401\n",
    "\n",
    "mlp_forward = inspect.getsource(LlamaMLP.forward)\n",
    "if \"apply_mx_linear\" not in mlp_forward:\n",
    "    print(\"Warning: apply_mx_linear not detected in LlamaMLP.forward source.\")\n",
    "else:\n",
    "    print(\"MX integration detected in LlamaMLP.forward.\")\n",
    "\n",
    "print(\"MX integration verification complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb0d8f1",
   "metadata": {
    "id": "7bb0d8f1"
   },
   "source": [
    "## Step 7: Quick Test (10% Dataset)\n",
    "\n",
    "Run a quick test to confirm the deployed code is the Trial 2 version and that `lm_eval` executes successfully.\n",
    "\n",
    "Estimated time: 1-2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2ce0f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying Trial 2 deployment in: /usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\n",
      "Trial 2 deployment verified\n",
      "\n",
      "+ lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32 --limit 0.1\n",
      "  env overrides: {'USE_MX_QUANTIZATION': '1', 'MX_W_ELEM_FORMAT': 'fp4_e2m1', 'MX_A_ELEM_FORMAT': 'fp6_e2m3', 'MX_BLOCK_SIZE': '32', 'MX_SCALE_BITS': '8', 'MX_SHARED_EXP_METHOD': 'max', 'MX_ROUND': 'even', 'MX_CUSTOM_CUDA': '1'}\n",
      "|lambada_openai|      1|none  |     0|acc       |↑  |0.5000|±  |0.0220|\n",
      "|              |       |none  |     0|perplexity|↓  |9.7963|±  |0.8769|\n"
     ]
    }
   ],
   "source": [
    "# Verify Trial 2 semantics are present in the deployed transformers file, then run a 10% eval.\n",
    "import ast\n",
    "import os\n",
    "import subprocess\n",
    "import transformers\n",
    "\n",
    "llama_file = transformers.models.llama.modeling_llama.__file__\n",
    "print(f\"Verifying Trial 2 deployment in: {llama_file}\")\n",
    "\n",
    "with open(llama_file, \"r\") as f:\n",
    "    deployed_src = f.read()\n",
    "\n",
    "def _trial2_semantics_present(src):\n",
    "    \"\"\"Returns (ok, reason). ok=True means bias and mx_specs are passed into the MX linear call.\"\"\"\n",
    "    try:\n",
    "        tree = ast.parse(src)\n",
    "    except SyntaxError as e:\n",
    "        return False, f\"Unable to parse deployed modeling_llama.py: {e}\"\n",
    "\n",
    "    apply_fn = None\n",
    "    for node in tree.body:\n",
    "        if isinstance(node, ast.FunctionDef) and node.name == \"apply_mx_linear\":\n",
    "            apply_fn = node\n",
    "            break\n",
    "    if apply_fn is None:\n",
    "        return False, \"apply_mx_linear not found\"\n",
    "\n",
    "    # Guard against Trial-1-style manual bias add after MX op.\n",
    "    for node in ast.walk(apply_fn):\n",
    "        if isinstance(node, ast.BinOp) and isinstance(node.op, ast.Add):\n",
    "            left_is_bias = isinstance(node.left, ast.Name) and node.left.id == \"bias\"\n",
    "            right_is_bias = isinstance(node.right, ast.Name) and node.right.id == \"bias\"\n",
    "            if left_is_bias or right_is_bias:\n",
    "                return False, \"Found a manual '+ bias' in apply_mx_linear (likely Trial 1)\"\n",
    "\n",
    "    def call_passes_bias_and_specs(call):\n",
    "        bias_ok = False\n",
    "        specs_ok = False\n",
    "\n",
    "        if len(call.args) >= 3 and isinstance(call.args[2], ast.Name) and call.args[2].id == \"bias\":\n",
    "            bias_ok = True\n",
    "        if len(call.args) >= 4 and isinstance(call.args[3], ast.Name) and call.args[3].id == \"mx_specs\":\n",
    "            specs_ok = True\n",
    "\n",
    "        for kw in call.keywords or []:\n",
    "            if kw.arg == \"bias\" and isinstance(kw.value, ast.Name) and kw.value.id == \"bias\":\n",
    "                bias_ok = True\n",
    "            if kw.arg in {\"mx_specs\", \"specs\"} and isinstance(kw.value, ast.Name) and kw.value.id == \"mx_specs\":\n",
    "                specs_ok = True\n",
    "\n",
    "        return bias_ok and specs_ok\n",
    "\n",
    "    for node in ast.walk(apply_fn):\n",
    "        if isinstance(node, ast.Call) and call_passes_bias_and_specs(node):\n",
    "            return True, \"\"\n",
    "\n",
    "    return False, \"No call found in apply_mx_linear that passes bias and mx_specs\"\n",
    "\n",
    "trial2_ok, trial2_reason = _trial2_semantics_present(deployed_src)\n",
    "if not trial2_ok:\n",
    "    raise RuntimeError(\n",
    "        \"Unable to confirm Trial 2 semantics in the deployed transformers file. \"\n",
    "        f\"Detail: {trial2_reason}. Re-run Step 4 (deploy) and restart the runtime, then try again.\"\n",
    "    )\n",
    "\n",
    "print(\"Trial 2 deployment verified\")\n",
    "\n",
    "# ----------------------\n",
    "# Quick eval runner that captures output and prints the final metrics table row.\n",
    "# ----------------------\n",
    "def run_lm_eval(env_overrides: dict, limit: float | None):\n",
    "    env = os.environ.copy()\n",
    "    env.update({k: str(v) for k, v in env_overrides.items()})\n",
    "    cmd = [\n",
    "        \"lm_eval\",\n",
    "        \"--model\", \"hf\",\n",
    "        \"--model_args\", \"pretrained=meta-llama/Llama-3.2-1B\",\n",
    "        \"--tasks\", \"lambada_openai\",\n",
    "        \"--device\", \"cuda\",\n",
    "        \"--batch_size\", \"32\",\n",
    "    ]\n",
    "    if limit is not None:\n",
    "        cmd += [\"--limit\", str(limit)]\n",
    "    print(\"\\n+\", \" \".join(cmd))\n",
    "    if env_overrides:\n",
    "        print(\"  env overrides:\", {k: env[k] for k in env_overrides})\n",
    "    out = subprocess.check_output(cmd, env=env, text=True, stderr=subprocess.STDOUT)\n",
    "    # Print the result row(s) for lambada_openai and perplexity for easy copy/paste\n",
    "    lines = out.splitlines()\n",
    "    for line in lines:\n",
    "        if \"|lambada_openai|\" in line or \"|              |\" in line and \"perplexity\" in line:\n",
    "            print(line)\n",
    "    return out\n",
    "\n",
    "# Default MX config (matches Exercise 1 target)\n",
    "mx_env = {\n",
    "    \"USE_MX_QUANTIZATION\": \"1\",\n",
    "    \"MX_W_ELEM_FORMAT\": \"fp4_e2m1\",\n",
    "    \"MX_A_ELEM_FORMAT\": \"fp6_e2m3\",\n",
    "    \"MX_BLOCK_SIZE\": \"32\",\n",
    "    \"MX_SCALE_BITS\": \"8\",\n",
    "    \"MX_SHARED_EXP_METHOD\": \"max\",\n",
    "    \"MX_ROUND\": \"even\",\n",
    "    \"MX_CUSTOM_CUDA\": \"1\",\n",
    "}\n",
    "\n",
    "_ = run_lm_eval(mx_env, limit=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0695d813",
   "metadata": {},
   "source": [
    "## Step 7b: MX Ablations & Quick Sweep (10% Dataset)\n",
    "\n",
    "Run a small set of high-signal ablations and spec sweeps on a 10% subset (`--limit 0.1`) to identify configs that recover accuracy before spending time on full evaluation.\n",
    "\n",
    "**What this runs**\n",
    "- Baseline quick check (MX off)\n",
    "- MX full (fp4 weights + fp6 activations)\n",
    "- Weights-only (activation quantization disabled)\n",
    "- Activations-only (weight quantization disabled)\n",
    "- Block size sweep: 8 / 16 / 32 / 64\n",
    "- Rounding sweep: even / nearest\n",
    "\n",
    "If a configuration is not supported by the installed MX build, it will be marked as `ERROR` and the sweep continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8c41de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+ lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32 --limit 0.1\n",
      "  env overrides: {'USE_MX_QUANTIZATION': '0'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+ lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32 --limit 0.1\n",
      "  env overrides: {'USE_MX_QUANTIZATION': '1', 'MX_W_ELEM_FORMAT': 'fp4_e2m1', 'MX_A_ELEM_FORMAT': 'fp6_e2m3', 'MX_BLOCK_SIZE': '32', 'MX_SCALE_BITS': '8', 'MX_SHARED_EXP_METHOD': 'max', 'MX_ROUND': 'even', 'MX_CUSTOM_CUDA': '1'}\n",
      "\n",
      "+ lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32 --limit 0.1\n",
      "  env overrides: {'USE_MX_QUANTIZATION': '1', 'MX_W_ELEM_FORMAT': 'fp4_e2m1', 'MX_A_ELEM_FORMAT': 'None', 'MX_BLOCK_SIZE': '32', 'MX_SCALE_BITS': '8', 'MX_SHARED_EXP_METHOD': 'max', 'MX_ROUND': 'even', 'MX_CUSTOM_CUDA': '1'}\n",
      "\n",
      "+ lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32 --limit 0.1\n",
      "  env overrides: {'USE_MX_QUANTIZATION': '1', 'MX_W_ELEM_FORMAT': 'None', 'MX_A_ELEM_FORMAT': 'fp6_e2m3', 'MX_BLOCK_SIZE': '32', 'MX_SCALE_BITS': '8', 'MX_SHARED_EXP_METHOD': 'max', 'MX_ROUND': 'even', 'MX_CUSTOM_CUDA': '1'}\n",
      "\n",
      "+ lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32 --limit 0.1\n",
      "  env overrides: {'USE_MX_QUANTIZATION': '1', 'MX_W_ELEM_FORMAT': 'fp4_e2m1', 'MX_A_ELEM_FORMAT': 'fp6_e2m3', 'MX_BLOCK_SIZE': '8', 'MX_SCALE_BITS': '8', 'MX_SHARED_EXP_METHOD': 'max', 'MX_ROUND': 'even', 'MX_CUSTOM_CUDA': '1'}\n",
      "\n",
      "+ lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32 --limit 0.1\n",
      "  env overrides: {'USE_MX_QUANTIZATION': '1', 'MX_W_ELEM_FORMAT': 'fp4_e2m1', 'MX_A_ELEM_FORMAT': 'fp6_e2m3', 'MX_BLOCK_SIZE': '16', 'MX_SCALE_BITS': '8', 'MX_SHARED_EXP_METHOD': 'max', 'MX_ROUND': 'even', 'MX_CUSTOM_CUDA': '1'}\n",
      "\n",
      "+ lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32 --limit 0.1\n",
      "  env overrides: {'USE_MX_QUANTIZATION': '1', 'MX_W_ELEM_FORMAT': 'fp4_e2m1', 'MX_A_ELEM_FORMAT': 'fp6_e2m3', 'MX_BLOCK_SIZE': '32', 'MX_SCALE_BITS': '8', 'MX_SHARED_EXP_METHOD': 'max', 'MX_ROUND': 'even', 'MX_CUSTOM_CUDA': '1'}\n",
      "\n",
      "+ lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32 --limit 0.1\n",
      "  env overrides: {'USE_MX_QUANTIZATION': '1', 'MX_W_ELEM_FORMAT': 'fp4_e2m1', 'MX_A_ELEM_FORMAT': 'fp6_e2m3', 'MX_BLOCK_SIZE': '64', 'MX_SCALE_BITS': '8', 'MX_SHARED_EXP_METHOD': 'max', 'MX_ROUND': 'even', 'MX_CUSTOM_CUDA': '1'}\n",
      "\n",
      "+ lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32 --limit 0.1\n",
      "  env overrides: {'USE_MX_QUANTIZATION': '1', 'MX_W_ELEM_FORMAT': 'fp4_e2m1', 'MX_A_ELEM_FORMAT': 'fp6_e2m3', 'MX_BLOCK_SIZE': '32', 'MX_SCALE_BITS': '8', 'MX_SHARED_EXP_METHOD': 'max', 'MX_ROUND': 'even', 'MX_CUSTOM_CUDA': '1'}\n",
      "\n",
      "+ lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32 --limit 0.1\n",
      "  env overrides: {'USE_MX_QUANTIZATION': '1', 'MX_W_ELEM_FORMAT': 'fp4_e2m1', 'MX_A_ELEM_FORMAT': 'fp6_e2m3', 'MX_BLOCK_SIZE': '32', 'MX_SCALE_BITS': '8', 'MX_SHARED_EXP_METHOD': 'max', 'MX_ROUND': 'nearest', 'MX_CUSTOM_CUDA': '1'}\n",
      "\n",
      "+ lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32 --limit 0.1\n",
      "  env overrides: {'USE_MX_QUANTIZATION': '1', 'MX_W_ELEM_FORMAT': 'fp4_e2m1', 'MX_A_ELEM_FORMAT': 'fp6_e2m3', 'MX_BLOCK_SIZE': '8', 'MX_SCALE_BITS': '8', 'MX_SHARED_EXP_METHOD': 'max', 'MX_ROUND': 'nearest', 'MX_CUSTOM_CUDA': '1'}\n",
      "\n",
      "+ lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32 --limit 0.1\n",
      "  env overrides: {'USE_MX_QUANTIZATION': '1', 'MX_W_ELEM_FORMAT': 'fp4_e2m1', 'MX_A_ELEM_FORMAT': 'fp6_e2m3', 'MX_BLOCK_SIZE': '8', 'MX_SCALE_BITS': '8', 'MX_SHARED_EXP_METHOD': 'max', 'MX_ROUND': 'nearest', 'MX_CUSTOM_CUDA': '0'}\n",
      "\n",
      "+ lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32 --limit 0.1\n",
      "  env overrides: {'USE_MX_QUANTIZATION': '1', 'MX_W_ELEM_FORMAT': 'fp4_e2m1', 'MX_A_ELEM_FORMAT': 'fp6_e2m3', 'MX_BLOCK_SIZE': '16', 'MX_SCALE_BITS': '8', 'MX_SHARED_EXP_METHOD': 'max', 'MX_ROUND': 'nearest', 'MX_CUSTOM_CUDA': '1'}\n",
      "\n",
      "+ lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32 --limit 0.1\n",
      "  env overrides: {'USE_MX_QUANTIZATION': '1', 'MX_W_ELEM_FORMAT': 'fp4_e2m1', 'MX_A_ELEM_FORMAT': 'fp6_e2m3', 'MX_BLOCK_SIZE': '32', 'MX_SCALE_BITS': '8', 'MX_SHARED_EXP_METHOD': 'mean', 'MX_ROUND': 'even', 'MX_CUSTOM_CUDA': '1'}\n",
      "\n",
      "============================================================================================\n",
      "MX QUICK SWEEP (limit=0.1)\n",
      "============================================================================================\n",
      "config                                                 acc    stderr  runtime(s)  status\n",
      "--------------------------------------------------------------------------------------------\n",
      "baseline (MX off)                                   0.6085    0.0215        53.3  OK\n",
      "MX full (fp4/fp6)                                   0.5000    0.0220        27.8  OK\n",
      "MX weights-only (a=None)                            0.5097    0.0220        32.5  OK\n",
      "MX activations-only (w=None)                        0.6085    0.0215        31.5  OK\n",
      "MX full bs=8                                        0.5174    0.0220        27.6  OK\n",
      "MX full bs=16                                       0.5097    0.0220        53.5  OK\n",
      "MX full bs=32                                       0.5000    0.0220        27.7  OK\n",
      "MX full bs=64                                       0.4961    0.0220        32.7  OK\n",
      "MX full round=even                                  0.5000    0.0220        28.2  OK\n",
      "MX full round=nearest                               0.5116    0.0220        28.2  OK\n",
      "MX full bs=8 round=nearest                          0.5194    0.0220        28.0  OK\n",
      "MX full bs=8 round=nearest (custom_cuda=0)          0.5291    0.0220        32.4  OK\n",
      "MX full bs=16 round=nearest                         0.5058    0.0220        28.7  OK\n",
      "MX full shared_exp=mean                             0.5000    0.0220        31.6  OK\n",
      "============================================================================================\n",
      "\n",
      "Best overall quick-sweep config:\n",
      "- name: baseline (MX off)\n",
      "- acc: 0.6085 ± 0.0215\n",
      "- env: {'USE_MX_QUANTIZATION': '0'}\n",
      "\n",
      "Best MX-enabled quick-sweep config (MX-any):\n",
      "- name: MX activations-only (w=None)\n",
      "- acc: 0.6085 ± 0.0215\n",
      "- env: {'USE_MX_QUANTIZATION': '1', 'MX_W_ELEM_FORMAT': 'None', 'MX_A_ELEM_FORMAT': 'fp6_e2m3', 'MX_BLOCK_SIZE': '32', 'MX_SCALE_BITS': '8', 'MX_SHARED_EXP_METHOD': 'max', 'MX_ROUND': 'even', 'MX_CUSTOM_CUDA': '1'}\n",
      "\n",
      "Best MX-full quick-sweep config (weights+activations):\n",
      "- name: MX full bs=8 round=nearest (custom_cuda=0)\n",
      "- acc: 0.5291 ± 0.0220\n",
      "- env: {'USE_MX_QUANTIZATION': '1', 'MX_W_ELEM_FORMAT': 'fp4_e2m1', 'MX_A_ELEM_FORMAT': 'fp6_e2m3', 'MX_BLOCK_SIZE': '8', 'MX_SCALE_BITS': '8', 'MX_SHARED_EXP_METHOD': 'max', 'MX_ROUND': 'nearest', 'MX_CUSTOM_CUDA': '0'}\n"
     ]
    }
   ],
   "source": [
    "# Quick sweep runner (10% limit)\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from dataclasses import asdict, dataclass\n",
    " \n",
    "@dataclass\n",
    "class SweepResult:\n",
    "    name: str\n",
    "    ok: bool\n",
    "    acc: float | None\n",
    "    acc_stderr: float | None\n",
    "    runtime_s: float | None\n",
    "    note: str\n",
    "    env: dict[str, str]\n",
    " \n",
    "def parse_lm_eval_acc(text: str) -> tuple[float | None, float | None]:\n",
    "    acc = None\n",
    "    acc_stderr = None\n",
    "    for line in text.splitlines():\n",
    "        if \"|lambada_openai|\" in line and \"|acc\" in line:\n",
    "            parts = [p.strip() for p in line.split(\"|\") if p.strip()]\n",
    "            try:\n",
    "                acc = float(parts[6])\n",
    "                acc_stderr = float(parts[8])\n",
    "            except Exception:\n",
    "                pass\n",
    "            break\n",
    "    return acc, acc_stderr\n",
    " \n",
    "def run_lm_eval_quick(env_overrides: dict[str, str], limit: float = 0.1) -> tuple[str, float | None, float | None, float]:\n",
    "    env = os.environ.copy()\n",
    "    env.update({k: str(v) for k, v in env_overrides.items()})\n",
    "    cmd = [\n",
    "        \"lm_eval\",\n",
    "        \"--model\", \"hf\",\n",
    "        \"--model_args\", \"pretrained=meta-llama/Llama-3.2-1B\",\n",
    "        \"--tasks\", \"lambada_openai\",\n",
    "        \"--device\", \"cuda\",\n",
    "        \"--batch_size\", \"32\",\n",
    "        \"--limit\", str(limit),\n",
    "    ]\n",
    "    print(\"\\n+\", \" \".join(cmd))\n",
    "    if env_overrides:\n",
    "        print(\"  env overrides:\", env_overrides)\n",
    "    t0 = time.time()\n",
    "    out = subprocess.check_output(cmd, env=env, text=True, stderr=subprocess.STDOUT)\n",
    "    runtime_s = time.time() - t0\n",
    "    acc, acc_stderr = parse_lm_eval_acc(out)\n",
    "    return out, acc, acc_stderr, runtime_s\n",
    " \n",
    "def _is_mx_enabled(env: dict[str, str]) -> bool:\n",
    "    return str(env.get(\"USE_MX_QUANTIZATION\", \"0\")) == \"1\"\n",
    " \n",
    "def _normalize_mx_format(v) -> str | None:\n",
    "    if v is None:\n",
    "        return None\n",
    "    s = str(v).strip()\n",
    "    if s == \"\":\n",
    "        return None\n",
    "    if s.lower() in {\"none\", \"null\"}:\n",
    "        return None\n",
    "    return s\n",
    " \n",
    "def _is_mx_full(env: dict[str, str]) -> bool:\n",
    "    if not _is_mx_enabled(env):\n",
    "        return False\n",
    "    w = _normalize_mx_format(env.get(\"MX_W_ELEM_FORMAT\", None))\n",
    "    a = _normalize_mx_format(env.get(\"MX_A_ELEM_FORMAT\", None))\n",
    "    return (w is not None) and (a is not None)\n",
    " \n",
    "# Base configs\n",
    "mx_base: dict[str, str] = {\n",
    "    \"USE_MX_QUANTIZATION\": \"1\",\n",
    "    \"MX_W_ELEM_FORMAT\": \"fp4_e2m1\",\n",
    "    \"MX_A_ELEM_FORMAT\": \"fp6_e2m3\",\n",
    "    \"MX_BLOCK_SIZE\": \"32\",\n",
    "    \"MX_SCALE_BITS\": \"8\",\n",
    "    \"MX_SHARED_EXP_METHOD\": \"max\",\n",
    "    \"MX_ROUND\": \"even\",\n",
    "    \"MX_CUSTOM_CUDA\": \"1\",\n",
    "}\n",
    " \n",
    "configs: list[tuple[str, dict[str, str]]] = []\n",
    "configs.append((\"baseline (MX off)\", {\"USE_MX_QUANTIZATION\": \"0\"}))\n",
    "configs.append((\"MX full (fp4/fp6)\", dict(mx_base)))\n",
    "configs.append((\"MX weights-only (a=None)\", {**mx_base, \"MX_A_ELEM_FORMAT\": \"None\"}))\n",
    "configs.append((\"MX activations-only (w=None)\", {**mx_base, \"MX_W_ELEM_FORMAT\": \"None\"}))\n",
    " \n",
    "# Block-size sweep\n",
    "for bs in [8, 16, 32, 64]:\n",
    "    configs.append((f\"MX full bs={bs}\", {**mx_base, \"MX_BLOCK_SIZE\": str(bs)}))\n",
    " \n",
    "# Rounding sweep\n",
    "for rnd in [\"even\", \"nearest\"]:\n",
    "    configs.append((f\"MX full round={rnd}\", {**mx_base, \"MX_ROUND\": rnd}))\n",
    " \n",
    "# Combo configs (best-of-both-worlds candidates)\n",
    "configs.append((\"MX full bs=8 round=nearest\", {**mx_base, \"MX_BLOCK_SIZE\": \"8\", \"MX_ROUND\": \"nearest\"}))\n",
    "configs.append((\"MX full bs=8 round=nearest (custom_cuda=0)\", {**mx_base, \"MX_BLOCK_SIZE\": \"8\", \"MX_ROUND\": \"nearest\", \"MX_CUSTOM_CUDA\": \"0\"}))\n",
    "configs.append((\"MX full bs=16 round=nearest\", {**mx_base, \"MX_BLOCK_SIZE\": \"16\", \"MX_ROUND\": \"nearest\"}))\n",
    " \n",
    "# Optional: small shared-exp sweep; unsupported values will be marked ERROR.\n",
    "for method in [\"max\", \"mean\"]:\n",
    "    if method == mx_base[\"MX_SHARED_EXP_METHOD\"]:\n",
    "        continue\n",
    "    configs.append((f\"MX full shared_exp={method}\", {**mx_base, \"MX_SHARED_EXP_METHOD\": method}))\n",
    " \n",
    "results: list[SweepResult] = []\n",
    "for name, env in configs:\n",
    "    try:\n",
    "        _, acc, acc_stderr, runtime_s = run_lm_eval_quick(env, limit=0.1)\n",
    "        ok = acc is not None\n",
    "        note = \"OK\" if ok else \"PARSE_FAIL\"\n",
    "        results.append(SweepResult(name=name, ok=ok, acc=acc, acc_stderr=acc_stderr, runtime_s=runtime_s, note=note, env=env))\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        results.append(SweepResult(name=name, ok=False, acc=None, acc_stderr=None, runtime_s=None, note=f\"ERROR: {e.returncode}\", env=env))\n",
    " \n",
    "# Print compact table\n",
    "print(\"\\n\" + \"=\" * 92)\n",
    "print(\"MX QUICK SWEEP (limit=0.1)\")\n",
    "print(\"=\" * 92)\n",
    "print(f\"{'config':48s}  {'acc':>8s}  {'stderr':>8s}  {'runtime(s)':>10s}  status\")\n",
    "print(\"-\" * 92)\n",
    "for r in results:\n",
    "    acc_s = \"<na>\" if r.acc is None else f\"{r.acc:.4f}\"\n",
    "    se_s = \"<na>\" if r.acc_stderr is None else f\"{r.acc_stderr:.4f}\"\n",
    "    rt_s = \"<na>\" if r.runtime_s is None else f\"{r.runtime_s:0.1f}\"\n",
    "    status = \"OK\" if r.ok else r.note\n",
    "    print(f\"{r.name[:48]:48s}  {acc_s:>8s}  {se_s:>8s}  {rt_s:>10s}  {status}\")\n",
    "print(\"=\" * 92)\n",
    " \n",
    "def _best_by_acc(candidates: list[SweepResult]) -> SweepResult | None:\n",
    "    best: SweepResult | None = None\n",
    "    for r in candidates:\n",
    "        if not r.ok or r.acc is None:\n",
    "            continue\n",
    "        if best is None or r.acc > best.acc:\n",
    "            best = r\n",
    "    return best\n",
    " \n",
    "best_any = _best_by_acc(results)\n",
    "best_mx_any = _best_by_acc([r for r in results if _is_mx_enabled(r.env)])\n",
    "best_mx_full = _best_by_acc([r for r in results if _is_mx_full(r.env)])\n",
    " \n",
    "if best_any is not None:\n",
    "    print(\"\\nBest overall quick-sweep config:\")\n",
    "    print(\"- name:\", best_any.name)\n",
    "    print(\"- acc:\", f\"{best_any.acc:.4f}\" + (\"\" if best_any.acc_stderr is None else f\" ± {best_any.acc_stderr:.4f}\"))\n",
    "    print(\"- env:\", best_any.env)\n",
    "else:\n",
    "    print(\"\\nNo successful quick-sweep configs found (all failed or unparsable).\")\n",
    " \n",
    "if best_mx_any is not None:\n",
    "    print(\"\\nBest MX-enabled quick-sweep config (MX-any):\")\n",
    "    print(\"- name:\", best_mx_any.name)\n",
    "    print(\"- acc:\", f\"{best_mx_any.acc:.4f}\" + (\"\" if best_mx_any.acc_stderr is None else f\" ± {best_mx_any.acc_stderr:.4f}\"))\n",
    "    print(\"- env:\", best_mx_any.env)\n",
    "else:\n",
    "    print(\"\\nNo successful MX-enabled configs found (all MX runs failed or unparsable).\")\n",
    " \n",
    "if best_mx_full is not None:\n",
    "    print(\"\\nBest MX-full quick-sweep config (weights+activations):\")\n",
    "    print(\"- name:\", best_mx_full.name)\n",
    "    print(\"- acc:\", f\"{best_mx_full.acc:.4f}\" + (\"\" if best_mx_full.acc_stderr is None else f\" ± {best_mx_full.acc_stderr:.4f}\"))\n",
    "    print(\"- env:\", best_mx_full.env)\n",
    "else:\n",
    "    print(\"\\nNo successful MX-full configs found (all MX-full runs failed or unparsable).\")\n",
    " \n",
    "# Persist to globals for later cells (Save Results)\n",
    "quick_sweep_results = [asdict(r) for r in results]\n",
    "quick_best_config_any = None if best_any is None else asdict(best_any)\n",
    "quick_best_config = None if best_mx_any is None else asdict(best_mx_any)\n",
    "quick_best_config_mx_full = None if best_mx_full is None else asdict(best_mx_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2dbe94",
   "metadata": {
    "id": "ce2dbe94"
   },
   "source": [
    "## Step 8: Full Evaluation (Exercise 1)\n",
    "\n",
    "Run the complete evaluation with the MX-quantized model.\n",
    "\n",
    "Estimated time: 10-15 minutes\n",
    "Baseline: 62.10% accuracy\n",
    "Target: > 60% accuracy (< 2% degradation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df3b4fa",
   "metadata": {
    "id": "5df3b4fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+ lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32\n",
      "  env overrides: {'USE_MX_QUANTIZATION': '0'}\n",
      "Parsed metrics: {'acc': 0.621, 'acc_stderr': 0.0068, 'perplexity': None, 'perplexity_stderr': None}\n",
      "\n",
      "+ lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32\n",
      "  env overrides: {'USE_MX_QUANTIZATION': '1', 'MX_W_ELEM_FORMAT': 'fp4_e2m1', 'MX_A_ELEM_FORMAT': 'fp6_e2m3', 'MX_BLOCK_SIZE': '32', 'MX_SCALE_BITS': '8', 'MX_SHARED_EXP_METHOD': 'max', 'MX_ROUND': 'even', 'MX_CUSTOM_CUDA': '1'}\n",
      "Parsed metrics: {'acc': 0.5067, 'acc_stderr': 0.007, 'perplexity': None, 'perplexity_stderr': None}\n",
      "\n",
      "+ lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32\n",
      "  env overrides: {'USE_MX_QUANTIZATION': '1', 'MX_W_ELEM_FORMAT': 'fp4_e2m1', 'MX_A_ELEM_FORMAT': 'fp6_e2m3', 'MX_BLOCK_SIZE': '32', 'MX_SCALE_BITS': '8', 'MX_SHARED_EXP_METHOD': 'max', 'MX_ROUND': 'even', 'MX_CUSTOM_CUDA': '0'}\n",
      "Parsed metrics: {'acc': 0.5084, 'acc_stderr': 0.007, 'perplexity': None, 'perplexity_stderr': None}\n",
      "\n",
      "+ lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32\n",
      "  env overrides: {'USE_MX_QUANTIZATION': '1', 'MX_W_ELEM_FORMAT': 'fp4_e2m1', 'MX_A_ELEM_FORMAT': 'None', 'MX_BLOCK_SIZE': '32', 'MX_SCALE_BITS': '8', 'MX_SHARED_EXP_METHOD': 'max', 'MX_ROUND': 'even', 'MX_CUSTOM_CUDA': '1'}\n",
      "Parsed metrics: {'acc': 0.5176, 'acc_stderr': 0.007, 'perplexity': None, 'perplexity_stderr': None}\n",
      "\n",
      "+ lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32\n",
      "  env overrides: {'USE_MX_QUANTIZATION': '1', 'MX_W_ELEM_FORMAT': 'None', 'MX_A_ELEM_FORMAT': 'fp6_e2m3', 'MX_BLOCK_SIZE': '32', 'MX_SCALE_BITS': '8', 'MX_SHARED_EXP_METHOD': 'max', 'MX_ROUND': 'even', 'MX_CUSTOM_CUDA': '1'}\n",
      "Parsed metrics: {'acc': 0.6161, 'acc_stderr': 0.0068, 'perplexity': None, 'perplexity_stderr': None}\n",
      "\n",
      "Summary (acc %):\n",
      "  baseline (MX off): 62.1\n",
      "  MX full: 50.67\n",
      "  MX full (custom_cuda=0): 50.839999999999996\n",
      "  MX weights-only: 51.76\n",
      "  MX activations-only: 61.61\n"
     ]
    }
   ],
   "source": [
    "# Full evaluation with MX quantization (with diagnostics and simple parsing).\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def parse_lm_eval_metrics(text: str) -> dict:\n",
    "    # Extract the main acc row (and perplexity if present) from the markdown table\n",
    "    acc = None\n",
    "    acc_stderr = None\n",
    "    ppl = None\n",
    "    ppl_stderr = None\n",
    "    for line in text.splitlines():\n",
    "        if \"|lambada_openai|\" in line and \"|acc\" in line:\n",
    "            # ... |acc|↑|0.5102|±|0.0070|\n",
    "            parts = [p.strip() for p in line.split(\"|\") if p.strip()]\n",
    "            # expect: lambada_openai, 1, none, 0, acc, ↑, value, ±, stderr\n",
    "            try:\n",
    "                acc = float(parts[6])\n",
    "                acc_stderr = float(parts[8])\n",
    "            except Exception:\n",
    "                pass\n",
    "        if \"perplexity\" in line and \"|              |\" in line:\n",
    "            parts = [p.strip() for p in line.split(\"|\") if p.strip()]\n",
    "            try:\n",
    "                ppl = float(parts[5])\n",
    "                ppl_stderr = float(parts[7])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return {\"acc\": acc, \"acc_stderr\": acc_stderr, \"perplexity\": ppl, \"perplexity_stderr\": ppl_stderr}\n",
    "\n",
    "\n",
    "def run_lm_eval(env_overrides: dict, limit: float | None):\n",
    "    env = os.environ.copy()\n",
    "    env.update({k: str(v) for k, v in env_overrides.items()})\n",
    "    cmd = [\n",
    "        \"lm_eval\",\n",
    "        \"--model\",\n",
    "        \"hf\",\n",
    "        \"--model_args\",\n",
    "        \"pretrained=meta-llama/Llama-3.2-1B\",\n",
    "        \"--tasks\",\n",
    "        \"lambada_openai\",\n",
    "        \"--device\",\n",
    "        \"cuda\",\n",
    "        \"--batch_size\",\n",
    "        \"32\",\n",
    "    ]\n",
    "    if limit is not None:\n",
    "        cmd += [\"--limit\", str(limit)]\n",
    "    print(\"\\n+\", \" \".join(cmd))\n",
    "    if env_overrides:\n",
    "        print(\"  env overrides:\", {k: env[k] for k in env_overrides})\n",
    "    out = subprocess.check_output(cmd, env=env, text=True, stderr=subprocess.STDOUT)\n",
    "    metrics = parse_lm_eval_metrics(out)\n",
    "    print(\"Parsed metrics:\", metrics)\n",
    "    return out, metrics\n",
    "\n",
    "\n",
    "# 1) Runtime baseline (MX disabled)\n",
    "baseline_out, baseline_metrics = run_lm_eval({\"USE_MX_QUANTIZATION\": \"0\"}, limit=None)\n",
    "\n",
    "# 2) MX full: prefer the best MX-full config from Step 7b if available\n",
    "best_mx_full = globals().get(\"quick_best_config_mx_full\", None)\n",
    "if isinstance(best_mx_full, dict) and isinstance(best_mx_full.get(\"env\"), dict) and best_mx_full.get(\"env\"):\n",
    "    mx_full_name = str(best_mx_full.get(\"name\", \"Best MX-full from Step 7b\"))\n",
    "    mx_full_env = {k: str(v) for k, v in best_mx_full[\"env\"].items()}\n",
    "else:\n",
    "    mx_full_name = \"MX default (no Step 7b winner)\"\n",
    "    mx_full_env = {\n",
    "        \"USE_MX_QUANTIZATION\": \"1\",\n",
    "        \"MX_W_ELEM_FORMAT\": \"fp4_e2m1\",\n",
    "        \"MX_A_ELEM_FORMAT\": \"fp6_e2m3\",\n",
    "        \"MX_BLOCK_SIZE\": \"32\",\n",
    "        \"MX_SCALE_BITS\": \"8\",\n",
    "        \"MX_SHARED_EXP_METHOD\": \"max\",\n",
    "        \"MX_ROUND\": \"even\",\n",
    "        \"MX_CUSTOM_CUDA\": \"1\",\n",
    "    }\n",
    "\n",
    "print(f\"\\nMX full-eval config: {mx_full_name}\")\n",
    "print(\"MX full-eval env:\", mx_full_env)\n",
    "\n",
    "mx_out, mx_metrics = run_lm_eval(mx_full_env, limit=None)\n",
    "\n",
    "# 3) MX full but custom CUDA disabled (diagnostic)\n",
    "mx_nocuda_out, mx_nocuda_metrics = run_lm_eval({**mx_full_env, \"MX_CUSTOM_CUDA\": \"0\"}, limit=None)\n",
    "\n",
    "# 4) MX weights-only (diagnostic)\n",
    "mx_wonly_out, mx_wonly_metrics = run_lm_eval({**mx_full_env, \"MX_A_ELEM_FORMAT\": \"None\"}, limit=None)\n",
    "\n",
    "# 5) MX activations-only (diagnostic)\n",
    "mx_aonly_out, mx_aonly_metrics = run_lm_eval({**mx_full_env, \"MX_W_ELEM_FORMAT\": \"None\"}, limit=None)\n",
    "\n",
    "# Persist for later cells\n",
    "full_eval_chosen_name = mx_full_name\n",
    "full_eval_chosen_env = mx_full_env\n",
    "\n",
    "baseline_acc = None if baseline_metrics[\"acc\"] is None else baseline_metrics[\"acc\"] * 100\n",
    "exercise1_acc = None if mx_metrics[\"acc\"] is None else mx_metrics[\"acc\"] * 100\n",
    "exercise1_acc_nocuda = None if mx_nocuda_metrics[\"acc\"] is None else mx_nocuda_metrics[\"acc\"] * 100\n",
    "exercise1_acc_wonly = None if mx_wonly_metrics[\"acc\"] is None else mx_wonly_metrics[\"acc\"] * 100\n",
    "exercise1_acc_aonly = None if mx_aonly_metrics[\"acc\"] is None else mx_aonly_metrics[\"acc\"] * 100\n",
    "\n",
    "print(\"\\nSummary (acc %):\")\n",
    "print(\"  baseline (MX off):\", baseline_acc)\n",
    "print(\"  MX full:\", exercise1_acc)\n",
    "print(\"  MX full (custom_cuda=0):\", exercise1_acc_nocuda)\n",
    "print(\"  MX weights-only:\", exercise1_acc_wonly)\n",
    "print(\"  MX activations-only:\", exercise1_acc_aonly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3f0c22",
   "metadata": {
    "id": "ae3f0c22"
   },
   "source": [
    "## Step 9: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa235f3",
   "metadata": {
    "id": "1fa235f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results appended to: /content/msr-intern-project/results/exercise1_results.txt\n"
     ]
    }
   ],
   "source": [
    "# Save Exercise 1 results (auto-filled if Step 8 and/or Step 7b ran)\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "baseline_acc_doc = 62.10\n",
    "baseline_acc_runtime = globals().get(\"baseline_acc\", None)\n",
    "baseline_acc = baseline_acc_runtime if baseline_acc_runtime is not None else baseline_acc_doc\n",
    "\n",
    "exercise1_acc = globals().get(\"exercise1_acc\", None)\n",
    "exercise1_acc_nocuda = globals().get(\"exercise1_acc_nocuda\", None)\n",
    "exercise1_acc_wonly = globals().get(\"exercise1_acc_wonly\", None)\n",
    "exercise1_acc_aonly = globals().get(\"exercise1_acc_aonly\", None)\n",
    "\n",
    "quick_sweep_results = globals().get(\"quick_sweep_results\", None)\n",
    "quick_best_config = globals().get(\"quick_best_config\", None)  # best MX-enabled (MX-any)\n",
    "quick_best_config_any = globals().get(\"quick_best_config_any\", None)\n",
    "quick_best_config_mx_full = globals().get(\"quick_best_config_mx_full\", None)\n",
    "\n",
    "full_eval_chosen_name = globals().get(\"full_eval_chosen_name\", None)\n",
    "full_eval_chosen_env = globals().get(\"full_eval_chosen_env\", None)\n",
    "\n",
    "\n",
    "def _fmt_pct(v):\n",
    "    return \"<missing>\" if v is None else f\"{v:.2f}%\"\n",
    "\n",
    "\n",
    "def _fmt_acc(v):\n",
    "    return \"<missing>\" if v is None else f\"{float(v):.4f}\"\n",
    "\n",
    "\n",
    "def _env_lines(env: dict | None) -> list[str]:\n",
    "    if not isinstance(env, dict) or not env:\n",
    "        return [\"- <missing>\"]\n",
    "    keys = [\n",
    "        \"USE_MX_QUANTIZATION\",\n",
    "        \"MX_W_ELEM_FORMAT\",\n",
    "        \"MX_A_ELEM_FORMAT\",\n",
    "        \"MX_BLOCK_SIZE\",\n",
    "        \"MX_SCALE_BITS\",\n",
    "        \"MX_SHARED_EXP_METHOD\",\n",
    "        \"MX_ROUND\",\n",
    "        \"MX_CUSTOM_CUDA\",\n",
    "    ]\n",
    "    lines: list[str] = []\n",
    "    for k in keys:\n",
    "        if k in env:\n",
    "            lines.append(f\"- {k}={env[k]}\")\n",
    "    # Include any other keys (stable order)\n",
    "    for k in sorted(set(env.keys()) - set(keys)):\n",
    "        lines.append(f\"- {k}={env[k]}\")\n",
    "    return lines\n",
    "\n",
    "\n",
    "repo_root = \"/content/msr-intern-project\"\n",
    "base_dir = repo_root if os.path.isdir(repo_root) else os.getcwd()\n",
    "\n",
    "# Canonical repo location for committed artifacts\n",
    "results_dir = os.path.join(base_dir, \"results\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "results_path = os.path.join(results_dir, \"exercise1_results.txt\")\n",
    "write_mode = \"a\" if os.path.exists(results_path) else \"w\"\n",
    "\n",
    "# Build an append-friendly report chunk\n",
    "lines: list[str] = []\n",
    "lines.append(\"\\n\" + \"=\" * 80)\n",
    "lines.append(f\"Notebook run: {timestamp}\")\n",
    "lines.append(\"=\" * 80)\n",
    "lines.append(\"Model: meta-llama/Llama-3.2-1B\")\n",
    "lines.append(\"Task: lambada_openai\")\n",
    "lines.append(\"Device: CUDA\")\n",
    "lines.append(\"Batch Size: 32\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"MX full-eval config (from Step 8):\")\n",
    "lines.append(f\"- name: {full_eval_chosen_name if full_eval_chosen_name else '<missing>'}\")\n",
    "lines.extend(_env_lines(full_eval_chosen_env))\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"Full-eval summary (if Step 8 ran):\")\n",
    "lines.append(f\"- Baseline (runtime, MX off): {_fmt_pct(baseline_acc_runtime)}\")\n",
    "lines.append(f\"- Baseline (doc):            {baseline_acc_doc:.2f}%\")\n",
    "lines.append(f\"- Using baseline:            {_fmt_pct(baseline_acc)}\")\n",
    "lines.append(f\"- MX full:                   {_fmt_pct(exercise1_acc)}\")\n",
    "lines.append(f\"- MX full (custom_cuda=0):   {_fmt_pct(exercise1_acc_nocuda)}\")\n",
    "lines.append(f\"- MX weights-only (a=None):  {_fmt_pct(exercise1_acc_wonly)}\")\n",
    "lines.append(f\"- MX activations-only (w=None): {_fmt_pct(exercise1_acc_aonly)}\")\n",
    "lines.append(\"\")\n",
    "\n",
    "if quick_sweep_results is not None:\n",
    "    lines.append(\"Quick sweep summary (Step 7b, limit=0.1):\")\n",
    "    lines.append(f\"{'config':48s}  {'acc':>8s}  {'stderr':>8s}  {'runtime(s)':>10s}  status\")\n",
    "    lines.append(\"-\" * 80)\n",
    "    for r in quick_sweep_results:\n",
    "        name = str(r.get(\"name\", \"\"))\n",
    "        acc = r.get(\"acc\", None)\n",
    "        acc_stderr = r.get(\"acc_stderr\", None)\n",
    "        runtime_s = r.get(\"runtime_s\", None)\n",
    "        ok = bool(r.get(\"ok\", False))\n",
    "        note = str(r.get(\"note\", \"\"))\n",
    "        acc_s = \"<na>\" if acc is None else f\"{float(acc):.4f}\"\n",
    "        se_s = \"<na>\" if acc_stderr is None else f\"{float(acc_stderr):.4f}\"\n",
    "        rt_s = \"<na>\" if runtime_s is None else f\"{float(runtime_s):0.1f}\"\n",
    "        status = \"OK\" if ok else note\n",
    "        lines.append(f\"{name[:48]:48s}  {acc_s:>8s}  {se_s:>8s}  {rt_s:>10s}  {status}\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    if quick_best_config_any is not None:\n",
    "        lines.append(\"Best quick-sweep config (overall):\")\n",
    "        lines.append(f\"- name: {quick_best_config_any.get('name')}\")\n",
    "        lines.append(f\"- acc:  {_fmt_acc(quick_best_config_any.get('acc'))}\")\n",
    "        lines.append(f\"- stderr: {_fmt_acc(quick_best_config_any.get('acc_stderr'))}\")\n",
    "        lines.append(f\"- env: {quick_best_config_any.get('env')}\")\n",
    "        lines.append(\"\")\n",
    "\n",
    "    if quick_best_config is not None:\n",
    "        lines.append(\"Best quick-sweep config (MX-enabled / MX-any):\")\n",
    "        lines.append(f\"- name: {quick_best_config.get('name')}\")\n",
    "        lines.append(f\"- acc:  {_fmt_acc(quick_best_config.get('acc'))}\")\n",
    "        lines.append(f\"- stderr: {_fmt_acc(quick_best_config.get('acc_stderr'))}\")\n",
    "        lines.append(f\"- env: {quick_best_config.get('env')}\")\n",
    "        lines.append(\"\")\n",
    "\n",
    "    if quick_best_config_mx_full is not None:\n",
    "        lines.append(\"Best quick-sweep config (MX-full / weights+activations):\")\n",
    "        lines.append(f\"- name: {quick_best_config_mx_full.get('name')}\")\n",
    "        lines.append(f\"- acc:  {_fmt_acc(quick_best_config_mx_full.get('acc'))}\")\n",
    "        lines.append(f\"- stderr: {_fmt_acc(quick_best_config_mx_full.get('acc_stderr'))}\")\n",
    "        lines.append(f\"- env: {quick_best_config_mx_full.get('env')}\")\n",
    "        lines.append(\"\")\n",
    "\n",
    "lines.append(\"Notes:\")\n",
    "lines.append(\"- Quick sweep uses --limit 0.1 and is for directional testing only.\")\n",
    "lines.append(\"- Full evaluation (no --limit) is the source of truth for final metrics.\")\n",
    "lines.append(\"\")\n",
    "\n",
    "report_chunk = \"\\n\".join(lines)\n",
    "with open(results_path, write_mode) as f:\n",
    "    f.write(report_chunk)\n",
    "\n",
    "print(f\"Results {'appended' if write_mode == 'a' else 'saved'} to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf4df52",
   "metadata": {
    "id": "7bf4df52"
   },
   "source": [
    "## Step 10: Analysis & Comparison\n",
    "\n",
    "Compare Exercise 1 results with baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a28967f",
   "metadata": {
    "id": "5a28967f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXERCISE 1 RESULTS ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Baseline Accuracy (runtime): 62.10%\n",
      "Baseline Accuracy (doc):     62.10%\n",
      "Using baseline:             62.10%\n",
      "\n",
      "MX full (fp4 weights + fp6 activations): 50.67%\n",
      "MX full (custom_cuda=0):              50.84%\n",
      "MX weights-only (fp4, a=None):        51.76%\n",
      "MX activations-only (fp6, w=None):    61.61%\n",
      "\n",
      "Change vs baseline (MX full): -11.43% (-18.41%)\n",
      "Result: exceeds 2% degradation threshold\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Comparison analysis (auto-filled from Step 8 if available)\n",
    "print(\"=\" * 70)\n",
    "print(\"EXERCISE 1 RESULTS ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    " \n",
    "def _fmt(v):\n",
    "    return \"<missing>\" if v is None else f\"{v:.2f}%\"\n",
    " \n",
    "# Prefer the runtime-measured baseline if present; fall back to the doc baseline.\n",
    "baseline_acc_doc = 62.10\n",
    "baseline_acc_runtime = globals().get(\"baseline_acc\", None)\n",
    "baseline_acc = baseline_acc_runtime if baseline_acc_runtime is not None else baseline_acc_doc\n",
    " \n",
    "exercise1_acc = globals().get(\"exercise1_acc\", None)\n",
    "exercise1_acc_nocuda = globals().get(\"exercise1_acc_nocuda\", None)\n",
    "exercise1_acc_wonly = globals().get(\"exercise1_acc_wonly\", None)\n",
    "exercise1_acc_aonly = globals().get(\"exercise1_acc_aonly\", None)\n",
    " \n",
    "print(f\"\\nBaseline Accuracy (runtime): {_fmt(baseline_acc_runtime)}\")\n",
    "print(f\"Baseline Accuracy (doc):     {baseline_acc_doc:.2f}%\")\n",
    "print(f\"Using baseline:             {_fmt(baseline_acc)}\")\n",
    " \n",
    "print(f\"\\nMX full (fp4 weights + fp6 activations): {_fmt(exercise1_acc)}\")\n",
    "print(f\"MX full (custom_cuda=0):              {_fmt(exercise1_acc_nocuda)}\")\n",
    "print(f\"MX weights-only (fp4, a=None):        {_fmt(exercise1_acc_wonly)}\")\n",
    "print(f\"MX activations-only (fp6, w=None):    {_fmt(exercise1_acc_aonly)}\")\n",
    " \n",
    "if exercise1_acc is not None:\n",
    "    accuracy_change = exercise1_acc - baseline_acc\n",
    "    accuracy_change_pct = (accuracy_change / baseline_acc) * 100\n",
    "    print(f\"\\nChange vs baseline (MX full): {accuracy_change:+.2f}% ({accuracy_change_pct:+.2f}%)\")\n",
    "    if accuracy_change >= -2.0:\n",
    "        print(\"Result: within target (< 2% degradation)\")\n",
    "    else:\n",
    "        print(\"Result: exceeds 2% degradation threshold\")\n",
    "else:\n",
    "    print(\"\\nRun Step 8 first to populate metrics.\")\n",
    " \n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ba5b3e",
   "metadata": {
    "id": "92ba5b3e"
   },
   "source": [
    "## Exercise 1 Wrap-up\n",
    "\n",
    "**Next steps**\n",
    "1. Record results in `results/exercise1_results.txt`\n",
    "2. Compare accuracy vs baseline (62.10%)\n",
    "3. Commit and push non-secret outputs (do not commit HF tokens)\n",
    "\n",
    "**Artifacts**\n",
    "- `Exercise1/modified_files/modeling_llama.py`\n",
    "- `Exercise1/mx_config_helper.py`\n",
    "- `results/exercise1_results.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872facb5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
