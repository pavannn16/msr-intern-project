{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7d197686",
      "metadata": {
        "id": "7d197686"
      },
      "source": [
        "# MSR Internship - Baseline Evaluation (Standalone)\n",
        "## Llama-3.2-1B on lambada_openai Task\n",
        "\n",
        "This notebook runs the baseline evaluation **without requiring GitHub authentication**.\n",
        "\n",
        "**Prerequisites:**\n",
        "- âœ… Google Colab with GPU runtime (T4/A100/H100)\n",
        "- âœ… Hugging Face account with Llama-3.2-1B access\n",
        "- âœ… HF access token\n",
        "\n",
        "**Expected Baseline Result:** ~62.24% accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0499d17c",
      "metadata": {
        "id": "0499d17c"
      },
      "source": [
        "## Step 1: Verify GPU Runtime\n",
        "\n",
        "âš ï¸ **IMPORTANT:** Before running any cells, make sure you're using a GPU runtime:\n",
        "1. Click the dropdown in top-right corner (near RAM/Disk)\n",
        "2. Select \"Change runtime type\"\n",
        "3. Choose **T4 GPU** (or A100/H100 if available)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "26239280",
      "metadata": {
        "id": "26239280",
        "outputId": "986e07eb-2b65-4c54-8aab-51840d990a4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jan 29 23:14:08 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0             45W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "208b40d6",
      "metadata": {
        "id": "208b40d6"
      },
      "source": [
        "## Step 2: Setup Environment\n",
        "\n",
        "â±ï¸ **Estimated time:** 3-5 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9010096e",
      "metadata": {
        "id": "9010096e",
        "outputId": "18c82ed2-56b8-4e4c-9ba1-80277e277886",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/transformers'...\n",
            "remote: Enumerating objects: 414197, done.\u001b[K\n",
            "remote: Counting objects: 100% (233/233), done.\u001b[K\n",
            "remote: Compressing objects: 100% (117/117), done.\u001b[K\n",
            "remote: Total 414197 (delta 173), reused 120 (delta 109), pack-reused 413964 (from 3)\u001b[K\n",
            "Receiving objects: 100% (414197/414197), 429.77 MiB | 35.26 MiB/s, done.\n",
            "Resolving deltas: 100% (319368/319368), done.\n",
            "Cloning into '/content/microxcaling'...\n",
            "remote: Enumerating objects: 274, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 274 (delta 37), reused 27 (delta 27), pack-reused 221 (from 1)\u001b[K\n",
            "Receiving objects: 100% (274/274), 963.57 KiB | 8.24 MiB/s, done.\n",
            "Resolving deltas: 100% (164/164), done.\n",
            "/content/transformers\n",
            "Note: switching to 'v4.57.6'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "HEAD is now at 753d611041 Release 4.57.6\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building editable for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\n",
            "âœ“ Setup complete!\n"
          ]
        }
      ],
      "source": [
        "# Clone transformers and microxcaling repositories\n",
        "!git clone https://github.com/huggingface/transformers.git /content/transformers\n",
        "!git clone https://github.com/microsoft/microxcaling.git /content/microxcaling\n",
        "\n",
        "# Checkout specific transformers version\n",
        "%cd /content/transformers\n",
        "!git checkout v4.57.6\n",
        "\n",
        "# Install transformers\n",
        "!pip install -e . -q\n",
        "\n",
        "# Install lm-eval and dependencies\n",
        "!pip install lm_eval ninja -q\n",
        "\n",
        "print(\"\\nâœ“ Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e820d9b5",
      "metadata": {
        "id": "e820d9b5"
      },
      "source": [
        "## Step 3: Set Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b7330427",
      "metadata": {
        "id": "b7330427",
        "outputId": "9987afb7-ffff-4993-f386-b0c061d4a424",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Environment variables set successfully\n"
          ]
        }
      ],
      "source": [
        "# Set Python path and HF token\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Add microxcaling to Python path\n",
        "sys.path.insert(0, '/content/microxcaling')\n",
        "os.environ['PYTHONPATH'] = '/content/microxcaling'\n",
        "\n",
        "# Set your Hugging Face token\n",
        "os.environ['HF_TOKEN'] = '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'\n",
        "\n",
        "print(\"âœ“ Environment variables set successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3629492b",
      "metadata": {
        "id": "3629492b"
      },
      "source": [
        "## Step 4: Quick Test (10% of Dataset)\n",
        "\n",
        "Test the setup with a small subset to verify everything works.\n",
        "\n",
        "â±ï¸ **Estimated time:** 1-2 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "13d0f773",
      "metadata": {
        "id": "13d0f773",
        "outputId": "c62816be-cdde-4ece-89bd-ce32d2ee2aa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-29:23:15:01 WARNING  [config.evaluate_config:281] --limit SHOULD ONLY BE USED FOR TESTING. REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
            "2026-01-29:23:15:07 INFO     [_cli.run:376] Selected Tasks: ['lambada_openai']\n",
            "2026-01-29:23:15:11 INFO     [evaluator:211] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "2026-01-29:23:15:11 INFO     [evaluator:236] Initializing hf model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B'}\n",
            "2026-01-29:23:15:19 INFO     [models.huggingface:161] Using device 'cuda'\n",
            "config.json: 100% 843/843 [00:00<00:00, 5.19MB/s]\n",
            "tokenizer_config.json: 100% 50.5k/50.5k [00:00<00:00, 4.67MB/s]\n",
            "tokenizer.json: 100% 9.09M/9.09M [00:00<00:00, 37.5MB/s]\n",
            "special_tokens_map.json: 100% 301/301 [00:00<00:00, 986kB/s]\n",
            "2026-01-29:23:15:20 INFO     [models.huggingface:423] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}\n",
            "2026-01-29 23:15:21.762889: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-01-29 23:15:21.782368: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769728521.802626    2498 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769728521.807977    2498 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769728521.822049    2498 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769728521.822080    2498 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769728521.822084    2498 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769728521.822088    2498 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-29 23:15:21.826305: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors: 100% 2.47G/2.47G [00:08<00:00, 300MB/s]\n",
            "generation_config.json: 100% 185/185 [00:00<00:00, 1.56MB/s]\n",
            "README.md: 5.47kB [00:00, 3.61MB/s]\n",
            "default/test/default.parquet: 100% 1.16M/1.16M [00:00<00:00, 3.63MB/s]\n",
            "Generating test split: 100% 5153/5153 [00:00<00:00, 123897.21 examples/s]\n",
            "2026-01-29:23:15:39 INFO     [tasks:700] Selected tasks:\n",
            "2026-01-29:23:15:39 INFO     [tasks:691] Task: lambada_openai (lambada/lambada_openai.yaml)\n",
            "2026-01-29:23:15:39 INFO     [api.task:311] Building contexts for lambada_openai on rank 0...\n",
            "100% 516/516 [00:01<00:00, 387.80it/s]\n",
            "2026-01-29:23:15:40 INFO     [evaluator:584] Running loglikelihood requests\n",
            "Running loglikelihood requests: 100% 516/516 [00:01<00:00, 267.92it/s]\n",
            "bootstrapping for stddev: perplexity\n",
            "100% 100/100 [00:00<00:00, 114.29it/s]\n",
            "2026-01-29:23:15:46 INFO     [loggers.evaluation_tracker:316] Output path not provided, skipping saving results aggregated\n",
            "hf ({'pretrained': 'meta-llama/Llama-3.2-1B'}), gen_kwargs: ({}), limit: 0.1, num_fewshot: None, batch_size: 32\n",
            "|    Tasks     |Version|Filter|n-shot|  Metric  |   |Value |   |Stderr|\n",
            "|--------------|------:|------|-----:|----------|---|-----:|---|-----:|\n",
            "|lambada_openai|      1|none  |     0|acc       |â†‘  |0.6085|Â±  |0.0215|\n",
            "|              |       |none  |     0|perplexity|â†“  |5.6636|Â±  |0.4281|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Quick test with 10% of the dataset\n",
        "!lm_eval --model hf \\\n",
        "  --model_args pretrained=meta-llama/Llama-3.2-1B \\\n",
        "  --tasks lambada_openai \\\n",
        "  --device cuda \\\n",
        "  --batch_size 32 \\\n",
        "  --limit 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a0fe5cd",
      "metadata": {
        "id": "0a0fe5cd"
      },
      "source": [
        "## Step 5: Full Baseline Evaluation\n",
        "\n",
        "Run the complete evaluation on 100% of the lambada_openai dataset.\n",
        "\n",
        "â±ï¸ **Estimated time:** 10-15 minutes  \n",
        "ğŸ¯ **Expected accuracy:** ~62.24%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5721ed82",
      "metadata": {
        "id": "5721ed82",
        "outputId": "f9525934-d68c-4137-9854-38bf6d9f5463",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-29:23:15:55 INFO     [_cli.run:376] Selected Tasks: ['lambada_openai']\n",
            "2026-01-29:23:15:56 INFO     [evaluator:211] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "2026-01-29:23:15:56 INFO     [evaluator:236] Initializing hf model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B'}\n",
            "2026-01-29:23:15:59 INFO     [models.huggingface:161] Using device 'cuda'\n",
            "2026-01-29:23:16:00 INFO     [models.huggingface:423] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}\n",
            "2026-01-29 23:16:01.008629: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-01-29 23:16:01.028644: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769728561.049902    2895 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769728561.055313    2895 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769728561.069242    2895 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769728561.069267    2895 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769728561.069270    2895 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769728561.069272    2895 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-29 23:16:01.073286: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2026-01-29:23:16:08 INFO     [tasks:700] Selected tasks:\n",
            "2026-01-29:23:16:08 INFO     [tasks:691] Task: lambada_openai (lambada/lambada_openai.yaml)\n",
            "2026-01-29:23:16:08 INFO     [api.task:311] Building contexts for lambada_openai on rank 0...\n",
            "100% 5153/5153 [00:13<00:00, 388.42it/s]\n",
            "2026-01-29:23:16:21 INFO     [evaluator:584] Running loglikelihood requests\n",
            "Running loglikelihood requests: 100% 5153/5153 [00:08<00:00, 612.48it/s]\n",
            "bootstrapping for stddev: perplexity\n",
            "100% 100/100 [00:08<00:00, 11.73it/s]\n",
            "2026-01-29:23:16:43 INFO     [loggers.evaluation_tracker:316] Output path not provided, skipping saving results aggregated\n",
            "hf ({'pretrained': 'meta-llama/Llama-3.2-1B'}), gen_kwargs: ({}), limit: None, num_fewshot: None, batch_size: 32\n",
            "|    Tasks     |Version|Filter|n-shot|  Metric  |   |Value |   |Stderr|\n",
            "|--------------|------:|------|-----:|----------|---|-----:|---|-----:|\n",
            "|lambada_openai|      1|none  |     0|acc       |â†‘  |0.6210|Â±  |0.0068|\n",
            "|              |       |none  |     0|perplexity|â†“  |5.4296|Â±  |0.1285|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Full baseline evaluation\n",
        "!lm_eval --model hf \\\n",
        "  --model_args pretrained=meta-llama/Llama-3.2-1B \\\n",
        "  --tasks lambada_openai \\\n",
        "  --device cuda \\\n",
        "  --batch_size 32"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5d16dd8",
      "metadata": {
        "id": "a5d16dd8"
      },
      "source": [
        "## Step 6: Create Results Directory and Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "185da9f8",
      "metadata": {
        "id": "185da9f8",
        "outputId": "e90e7158-94b0-4848-99ff-31b745aed3eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Results template saved to /content/results/baseline_results.txt\n",
            "\n",
            "âš ï¸ Remember to:\n",
            "  1. Copy the actual accuracy and runtime from the output above\n",
            "  2. Update the results file\n",
            "  3. Download the file from Colab (Files panel on left)\n"
          ]
        }
      ],
      "source": [
        "# Create results directory\n",
        "!mkdir -p /content/results\n",
        "\n",
        "# Save baseline results to file\n",
        "import datetime\n",
        "\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "with open('/content/results/baseline_results.txt', 'w') as f:\n",
        "    f.write(f\"Baseline Evaluation Results\\n\")\n",
        "    f.write(f\"=\" * 50 + \"\\n\")\n",
        "    f.write(f\"Timestamp: {timestamp}\\n\")\n",
        "    f.write(f\"Model: meta-llama/Llama-3.2-1B\\n\")\n",
        "    f.write(f\"Task: lambada_openai\\n\")\n",
        "    f.write(f\"Device: CUDA\\n\")\n",
        "    f.write(f\"Batch Size: 32\\n\")\n",
        "    f.write(f\"\\n\")\n",
        "    f.write(f\"Results:\\n\")\n",
        "    f.write(f\"Accuracy: [TO BE FILLED FROM ABOVE OUTPUT]\\n\")\n",
        "    f.write(f\"Runtime: [TO BE FILLED FROM ABOVE OUTPUT]\\n\")\n",
        "    f.write(f\"\\n\")\n",
        "    f.write(f\"NOTE: Manually update this file with the actual results from the evaluation output above.\\n\")\n",
        "\n",
        "print(\"âœ“ Results template saved to /content/results/baseline_results.txt\")\n",
        "print(\"\\nâš ï¸ Remember to:\")\n",
        "print(\"  1. Copy the actual accuracy and runtime from the output above\")\n",
        "print(\"  2. Update the results file\")\n",
        "print(\"  3. Download the file from Colab (Files panel on left)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c10cec34",
      "metadata": {
        "id": "c10cec34"
      },
      "source": [
        "## âœ… Baseline Evaluation Complete!\n",
        "\n",
        "### Next Steps:\n",
        "1. **Record your results** - Note the accuracy and runtime from Step 5\n",
        "2. **Compare with expected** - Should be around 62.24% accuracy\n",
        "3. **Download results** - Use Files panel on left to download results file\n",
        "4. **Move to Exercise 1** - Start implementing MX quantization for linear layers\n",
        "\n",
        "### Troubleshooting:\n",
        "- **HF Token Error**: Verify token has Read permission and Llama-3.2-1B access approved\n",
        "- **CUDA Error**: Check GPU runtime is enabled (T4/A100/H100)\n",
        "- **Out of Memory**: Reduce batch_size to 16 or 8\n",
        "- **Import Errors**: Re-run Step 2 (setup)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}