{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ac782294",
      "metadata": {
        "id": "ac782294"
      },
      "source": [
        "# Exercise 1: MX Quantization of Linear Layers\\n\n",
        "## Llama-3.2-1B with mxfp4_e2m1 (weights) + mxfp6_e2m3 (activations)\\n\n",
        "\\n\n",
        "This notebook evaluates the MX-quantized Llama model on the lambada_openai task.\\n\n",
        "\\n\n",
        "**Exercise Objectives:**\\n\n",
        "- ‚úÖ Quantize all linear layers (Q, K, V, O, gate, up, down)\\n\n",
        "- ‚úÖ Use mxfp4_e2m1 for weights (4-bit)\\n\n",
        "- ‚úÖ Use mxfp6_e2m3 for activations (6-bit)\\n\n",
        "- ‚úÖ Compare accuracy vs baseline (62.10%)\\n\n",
        "\\n\n",
        "**Expected Outcomes:**\\n\n",
        "- Memory savings: ~75% for weights, ~81% for activations\\n\n",
        "- Accuracy target: > 60% (< 2% degradation)\\n\n",
        "\\n\n",
        "**Author:** Pavan Chauhan  \\n\n",
        "**Date:** January 29, 2026"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2e976ce",
      "metadata": {
        "id": "d2e976ce"
      },
      "source": [
        "## Step 1: Verify GPU Runtime\\n\n",
        "\\n\n",
        "‚ö†Ô∏è **IMPORTANT:** Ensure GPU runtime is enabled (T4/A100/H100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9d66bb5f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d66bb5f",
        "outputId": "cf60df34-dfc9-4a97-cfd0-f16c25b8b656"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jan 30 16:02:02 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0             43W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\\n\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "034bdaa7",
      "metadata": {
        "id": "034bdaa7"
      },
      "source": [
        "## Step 2: Clone Project Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2ad10851",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ad10851",
        "outputId": "6f830cf8-4b91-4c5b-f5e9-873a00cbf69a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'msr-intern-project' already exists and is not an empty directory.\n",
            "/content/msr-intern-project\n"
          ]
        }
      ],
      "source": [
        "# Clone the project repository\n",
        "!git clone https://github.com/pavannn16/msr-intern-project.git\n",
        "%cd msr-intern-project"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be5f2e23",
      "metadata": {
        "id": "be5f2e23"
      },
      "source": [
        "## Step 3: Run Base Setup\\n\n",
        "\\n\n",
        "This installs transformers, microxcaling, and lm-eval.\\n\n",
        "\\n\n",
        "‚è±Ô∏è **Estimated time:** 3-5 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ee93de6e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee93de6e",
        "outputId": "bf3f9747-00b7-406b-d8e6-4602e9b792c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================\n",
            "MSR Internship Exercise - Setup Script\n",
            "======================================\n",
            "\n",
            "[1/5] Cloning transformers repository...\n",
            "‚úì Transformers already exists\n",
            "\n",
            "[2/5] Cloning microxcaling repository...\n",
            "‚úì Microxcaling already exists\n",
            "\n",
            "[3/5] Installing transformers...\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building editable for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "‚úì Transformers installed\n",
            "\n",
            "[4/5] Installing lm-eval and dependencies...\n",
            "‚úì lm-eval and ninja installed\n",
            "\n",
            "[5/5] Setting up environment variables...\n",
            "‚úì PYTHONPATH set to include microxcaling\n",
            "\n",
            "======================================\n",
            "Setup Complete!\n",
            "======================================\n",
            "\n",
            "‚ö†Ô∏è  IMPORTANT: Set your HF_TOKEN before running evaluations:\n",
            "   export HF_TOKEN=<your_huggingface_token>\n",
            "\n",
            "üìù To test the setup, run:\n",
            "   lm_eval --model hf \\\n",
            "     --model_args pretrained=meta-llama/Llama-3.2-1B \\\n",
            "     --tasks lambada_openai \\\n",
            "     --device cuda \\\n",
            "     --batch_size 32\n",
            "\n",
            "üí° Tip: Use --limit 0.1 for quick testing (10% of dataset)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run base setup (transformers + microxcaling)\\n\n",
        "!bash scripts/setup_colab.sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix transformers installation\n",
        "print(\"Fixing transformers installation...\")\n",
        "\n",
        "# Uninstall editable transformers\n",
        "!pip uninstall -y transformers\n",
        "\n",
        "# Install regular transformers from PyPI\n",
        "!pip install transformers==4.57.6\n",
        "\n",
        "print(\"‚úì Transformers reinstalled from PyPI\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DS0H7GZno-R3",
        "outputId": "21370c77-511e-4ec4-cba4-ac7eee5fa585"
      },
      "id": "DS0H7GZno-R3",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixing transformers installation...\n",
            "Found existing installation: transformers 4.57.6\n",
            "Uninstalling transformers-4.57.6:\n",
            "  Successfully uninstalled transformers-4.57.6\n",
            "Collecting transformers==4.57.6\n",
            "  Using cached transformers-4.57.6-py3-none-any.whl.metadata (43 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.6) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.6) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.6) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.6) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.6) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.6) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.6) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.6) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.6) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.6) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.6) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.6) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.6) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.6) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.6) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.6) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.6) (2026.1.4)\n",
            "Using cached transformers-4.57.6-py3-none-any.whl (12.0 MB)\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.57.6\n",
            "‚úì Transformers reinstalled from PyPI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Restore the PyPI transformers modeling_llama.py\n",
        "import shutil\n",
        "\n",
        "# Remove the editable install directory if it exists\n",
        "editable_path = \"/content/transformers\"\n",
        "if os.path.exists(editable_path):\n",
        "    print(f\"Removing editable install: {editable_path}\")\n",
        "    shutil.rmtree(editable_path)\n",
        "    print(\"‚úì Removed\")\n",
        "\n",
        "# Now Python will use the PyPI installed transformers\n",
        "print(\"\\n‚úì Will use PyPI transformers (properly installed)\")\n",
        "print(\"‚ö†Ô∏è  IMPORTANT: Restart runtime now (Runtime ‚Üí Restart runtime)\")\n",
        "print(\"Then skip Step 4 and continue from Step 5\")"
      ],
      "metadata": {
        "id": "PsTaoLxWpsGP",
        "outputId": "73b4c9a4-a389-4115-af0d-7862e26a0547",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PsTaoLxWpsGP",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing editable install: /content/transformers\n",
            "‚úì Removed\n",
            "\n",
            "‚úì Will use PyPI transformers (properly installed)\n",
            "‚ö†Ô∏è  IMPORTANT: Restart runtime now (Runtime ‚Üí Restart runtime)\n",
            "Then skip Step 4 and continue from Step 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "683f5af5",
      "metadata": {
        "id": "683f5af5"
      },
      "source": [
        "## Step 4: Run Exercise 1 Setup\\n\n",
        "\\n\n",
        "This:\\n\n",
        "- Verifies MX library installation\\n\n",
        "- Copies MX-quantized modeling_llama.py\\n\n",
        "- Sets up Exercise 1 environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "afda6634",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afda6634",
        "outputId": "db649ddd-502e-4026-fc50-8d9ead791578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================\n",
            "Exercise 1: MX Linear Layer Quantization\n",
            "==========================================\n",
            "\n",
            "[1/7] Checking base dependencies...\n",
            "  ‚Üí Transformers not found. Running base setup...\n",
            "======================================\n",
            "MSR Internship Exercise - Setup Script\n",
            "======================================\n",
            "\n",
            "[1/5] Cloning transformers repository...\n",
            "Cloning into '/content/transformers'...\n",
            "remote: Enumerating objects: 415595, done.\u001b[K\n",
            "remote: Counting objects: 100% (631/631), done.\u001b[K\n",
            "remote: Compressing objects: 100% (339/339), done.\u001b[K\n",
            "remote: Total 415595 (delta 466), reused 300 (delta 282), pack-reused 414964 (from 3)\u001b[K\n",
            "Receiving objects: 100% (415595/415595), 432.26 MiB | 35.32 MiB/s, done.\n",
            "Resolving deltas: 100% (320208/320208), done.\n",
            "Note: switching to 'v4.57.6'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "HEAD is now at 753d611041 Release 4.57.6\n",
            "‚úì Transformers cloned and checked out to v4.57.6\n",
            "\n",
            "[2/5] Cloning microxcaling repository...\n",
            "‚úì Microxcaling already exists\n",
            "\n",
            "[3/5] Installing transformers...\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building editable for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "‚úì Transformers installed\n",
            "\n",
            "[4/5] Installing lm-eval and dependencies...\n",
            "‚úì lm-eval and ninja installed\n",
            "\n",
            "[5/5] Setting up environment variables...\n",
            "‚úì PYTHONPATH set to include microxcaling\n",
            "\n",
            "======================================\n",
            "Setup Complete!\n",
            "======================================\n",
            "\n",
            "‚ö†Ô∏è  IMPORTANT: Set your HF_TOKEN before running evaluations:\n",
            "   export HF_TOKEN=<your_huggingface_token>\n",
            "\n",
            "üìù To test the setup, run:\n",
            "   lm_eval --model hf \\\n",
            "     --model_args pretrained=meta-llama/Llama-3.2-1B \\\n",
            "     --tasks lambada_openai \\\n",
            "     --device cuda \\\n",
            "     --batch_size 32\n",
            "\n",
            "üí° Tip: Use --limit 0.1 for quick testing (10% of dataset)\n",
            "\n",
            "  ‚úì Microxcaling found\n",
            "\n",
            "[2/7] Setting up Python path...\n",
            "  ‚úì Added /content/microxcaling to PYTHONPATH\n",
            "  ‚úì Added /content/msr-intern-project/Exercise1 to PYTHONPATH\n",
            "\n",
            "[3/7] Verifying MX library installation...\n",
            "  ‚úì MX library imports successful\n",
            "\n",
            "[4/7] Creating Exercise 1 directory structure...\n",
            "  ‚úì Directory structure created\n",
            "\n",
            "[5/7] Checking for modified modeling_llama files...\n",
            "  ‚úì Template file found: modeling_llama_mx_template.py\n",
            "  ‚Üí Creating backup of original file...\n",
            "  ‚úì Backup created: modeling_llama.py.backup\n",
            "  ‚Üí Copying MX-integrated file to transformers...\n",
            "  ‚úì MX-integrated modeling_llama.py deployed\n",
            "\n",
            "[6/7] Testing modified model import...\n",
            "2026-01-30 16:05:26.534730: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769789126.555886    7777 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769789126.562407    7777 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769789126.578673    7777 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769789126.578699    7777 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769789126.578702    7777 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769789126.578705    7777 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "MX Quantization Configuration:\n",
            "==================================================\n",
            "Weights: fp4_e2m1 (4-bit)\n",
            "Activations: fp6_e2m3 (6-bit)\n",
            "Scale Bits: 8 (E8M0)\n",
            "Block Size: 32\n",
            "CUDA Backend: Enabled\n",
            "Rounding: nearest\n",
            "Backward Quantization: Disabled\n",
            "==================================================\n",
            "  ‚ö† Import warning: cannot import name 'LlamaModel' from 'transformers.models.llama.modeling_llama' (/content/transformers/src/transformers/models/llama/modeling_llama.py)\n",
            "  This may be normal if model hasn't been modified yet\n",
            "\n",
            "[7/7] Verifying CUDA availability...\n",
            "  ‚úì CUDA available: NVIDIA A100-SXM4-40GB\n",
            "  ‚úì CUDA version: 12.6\n",
            "\n",
            "==========================================\n",
            "Setup Complete!\n",
            "==========================================\n",
            "\n",
            "Environment Configuration:\n",
            "  - Transformers: /content/transformers\n",
            "  - Microxcaling: /content/microxcaling\n",
            "  - Exercise 1: /content/msr-intern-project/Exercise1\n",
            "\n",
            "Next Steps:\n",
            "  1. Verify HF_TOKEN is set: echo $HF_TOKEN\n",
            "  2. Run evaluation with Exercise 1 notebook or:\n",
            "\n",
            "     lm_eval --model hf \\\n",
            "       --model_args pretrained=meta-llama/Llama-3.2-1B \\\n",
            "       --tasks lambada_openai \\\n",
            "       --device cuda \\\n",
            "       --batch_size 32\n",
            "\n",
            "  3. Compare results with baseline (62.10% accuracy)\n",
            "\n",
            "Troubleshooting:\n",
            "  - If imports fail: Check PYTHONPATH includes /content/microxcaling\n",
            "  - If CUDA errors: Ensure GPU runtime is selected\n",
            "  - If model errors: Verify modified file was copied correctly\n",
            "\n",
            "To restore original model:\n",
            "  cp /content/transformers/src/transformers/models/llama/modeling_llama.py.backup \\\n",
            "     /content/transformers/src/transformers/models/llama/modeling_llama.py\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run Exercise 1 specific setup\\n\n",
        "!bash Exercise1/scripts/setup_exercise1.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea5efce1",
      "metadata": {
        "id": "ea5efce1"
      },
      "source": [
        "## Step 5: Set Environment Variables\n",
        "\n",
        "‚ö†Ô∏è **IMPORTANT:** Add your Hugging Face token to Colab secrets:\n",
        "1. Click the üîë key icon in the left sidebar (Secrets)\n",
        "2. Add a new secret:\n",
        "   - **Name:** `HF_TOKEN`\n",
        "   - **Value:** Your Hugging Face token\n",
        "3. Enable notebook access for the secret\n",
        "\n",
        "Get your token at: https://huggingface.co/settings/tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e6c5fc44",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6c5fc44",
        "outputId": "6712b42d-927b-47ca-eefc-34260ed9a2f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì HF token retrieved from Colab secrets\n",
            "‚úì Environment variables configured\n",
            "  PYTHONPATH: /content/microxcaling:/content/msr-intern-project/Exercise1\n",
            "  USE_MX_QUANTIZATION: 1\n"
          ]
        }
      ],
      "source": [
        "# Set environment variables\n",
        "import os\n",
        "import sys\n",
        "from google.colab import userdata\n",
        "\n",
        "# Add paths\n",
        "sys.path.insert(0, '/content/microxcaling')\n",
        "sys.path.insert(0, '/content/msr-intern-project/Exercise1')\n",
        "\n",
        "os.environ['PYTHONPATH'] = '/content/microxcaling:/content/msr-intern-project/Exercise1'\n",
        "\n",
        "# Get HF token from Colab secrets\n",
        "try:\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    os.environ['HF_TOKEN'] = hf_token\n",
        "    print(\"‚úì HF token retrieved from Colab secrets\")\n",
        "except Exception as e:\n",
        "    print(\"‚ùå ERROR: Failed to retrieve HF token\")\n",
        "    print(\"Please add your Hugging Face token to Colab secrets:\")\n",
        "    print(\"1. Click the üîë key icon in the left sidebar\")\n",
        "    print(\"2. Add secret: Name='HF_TOKEN', Value=your_hf_token\")\n",
        "    raise\n",
        "\n",
        "os.environ['USE_MX_QUANTIZATION'] = '1'  # Enable MX quantization\n",
        "\n",
        "print(\"‚úì Environment variables configured\")\n",
        "print(f\"  PYTHONPATH: {os.environ['PYTHONPATH']}\")\n",
        "print(f\"  USE_MX_QUANTIZATION: {os.environ['USE_MX_QUANTIZATION']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a6e4420b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6e4420b",
        "outputId": "fdadca08-5a14-4a54-cd44-88e216802cf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing MX library import...\n",
            "‚úì MX library imported successfully\n",
            "\n",
            "Testing Exercise 1 helper module...\n",
            "MX Quantization Configuration:\n",
            "==================================================\n",
            "Weights: fp4_e2m1 (4-bit)\n",
            "Activations: fp6_e2m3 (6-bit)\n",
            "Scale Bits: 8 (E8M0)\n",
            "Block Size: 32\n",
            "CUDA Backend: Enabled\n",
            "Rounding: nearest\n",
            "Backward Quantization: Disabled\n",
            "==================================================\n",
            "\n",
            "Testing transformers installation...\n",
            "‚úì Transformers v4.57.6 installed\n",
            "\n",
            "Testing MX-integrated Llama model...\n",
            "‚úì Modified Llama model classes imported successfully\n",
            "‚ö†Ô∏è  Warning: MX quantization not detected in LlamaMLP\n",
            "\n",
            "==================================================\n",
            "‚úì ALL MX INTEGRATION TESTS PASSED\n",
            "==================================================\n",
            "\n",
            "Ready for evaluation! MX quantization will be applied during model loading.\n"
          ]
        }
      ],
      "source": [
        "## Step 6: Verify MX Integration\n",
        "\n",
        "# Test that MX library and modified model load correctly.\n",
        "# Test MX library import\n",
        "print(\"Testing MX library import...\")\n",
        "try:\n",
        "    from mx.specs import MxSpecs\n",
        "    from mx import linear as mx_linear\n",
        "    print(\"‚úì MX library imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå ERROR: MX library import failed: {e}\")\n",
        "    print(\"Please ensure the base setup (Step 3) completed successfully\")\n",
        "    raise\n",
        "\n",
        "# Test helper module\n",
        "print(\"\\nTesting Exercise 1 helper module...\")\n",
        "try:\n",
        "    from mx_config_helper import create_mx_specs_exercise1, print_mx_specs_summary\n",
        "    mx_specs = create_mx_specs_exercise1()\n",
        "    print_mx_specs_summary(mx_specs)\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå ERROR: Helper module import failed: {e}\")\n",
        "    print(\"Please ensure Exercise 1 setup (Step 4) completed successfully\")\n",
        "    raise\n",
        "\n",
        "# Test transformers installation\n",
        "print(\"\\nTesting transformers installation...\")\n",
        "try:\n",
        "    import transformers\n",
        "    # Get version safely (handle different transformers versions)\n",
        "    try:\n",
        "        version = transformers.__version__\n",
        "    except AttributeError:\n",
        "        # Try alternative method for older installations\n",
        "        try:\n",
        "            import importlib.metadata\n",
        "            version = importlib.metadata.version('transformers')\n",
        "        except:\n",
        "            version = \"unknown\"\n",
        "    print(f\"‚úì Transformers v{version} installed\")\n",
        "\n",
        "    # Test modified model import\n",
        "    print(\"\\nTesting MX-integrated Llama model...\")\n",
        "    from transformers.models.llama.modeling_llama import LlamaForCausalLM, LlamaMLP, LlamaAttention\n",
        "    print(\"‚úì Modified Llama model classes imported successfully\")\n",
        "\n",
        "    # Check if MX integration is present\n",
        "    import inspect\n",
        "    mlp_source = inspect.getsource(LlamaMLP.forward)\n",
        "    if 'apply_mx_linear' in mlp_source:\n",
        "        print(\"‚úì MX quantization detected in LlamaMLP\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  Warning: MX quantization not detected in LlamaMLP\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå ERROR: Model import failed: {e}\")\n",
        "    raise\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"‚úì ALL MX INTEGRATION TESTS PASSED\")\n",
        "print(\"=\" * 50)\n",
        "print(\"\\nReady for evaluation! MX quantization will be applied during model loading.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b881422",
      "metadata": {
        "id": "9b881422"
      },
      "source": [
        "## Step 7: Quick Test (10% Dataset)\\n\n",
        "\\n\n",
        "Run a quick test to verify everything works.\\n\n",
        "\\n\n",
        "‚è±Ô∏è **Estimated time:** 1-2 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35eeb822",
      "metadata": {
        "id": "35eeb822"
      },
      "outputs": [],
      "source": [
        "# Quick test with 10% of dataset\\n\n",
        "!lm_eval --model hf \\\\\\n\n",
        "  --model_args pretrained=meta-llama/Llama-3.2-1B \\\\\\n\n",
        "  --tasks lambada_openai \\\\\\n\n",
        "  --device cuda \\\\\\n\n",
        "  --batch_size 32 \\\\\\n\n",
        "  --limit 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62b0b764",
      "metadata": {
        "id": "62b0b764"
      },
      "source": [
        "## Step 8: Full Evaluation (Exercise 1)\\n\n",
        "\\n\n",
        "Run complete evaluation with MX-quantized model.\\n\n",
        "\\n\n",
        "‚è±Ô∏è **Estimated time:** 10-15 minutes  \\n\n",
        "üéØ **Baseline:** 62.10% accuracy  \\n\n",
        "üéØ **Target:** > 60% accuracy (< 2% degradation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25b6ad5c",
      "metadata": {
        "id": "25b6ad5c"
      },
      "outputs": [],
      "source": [
        "# Full evaluation with MX quantization\\n\n",
        "!lm_eval --model hf \\\\\\n\n",
        "  --model_args pretrained=meta-llama/Llama-3.2-1B \\\\\\n\n",
        "  --tasks lambada_openai \\\\\\n\n",
        "  --device cuda \\\\\\n\n",
        "  --batch_size 32"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb8ef11c",
      "metadata": {
        "id": "fb8ef11c"
      },
      "source": [
        "## Step 9: Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00581ab4",
      "metadata": {
        "id": "00581ab4"
      },
      "outputs": [],
      "source": [
        "# Save Exercise 1 results\n",
        "import datetime\n",
        "\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "results_content = f\"\"\"Exercise 1 Evaluation Results\n",
        "==================================================\n",
        "Timestamp: {timestamp}\n",
        "Model: meta-llama/Llama-3.2-1B\n",
        "Task: lambada_openai\n",
        "Device: CUDA\n",
        "Batch Size: 32\n",
        "\n",
        "MX Quantization Configuration:\n",
        "- Weight Format: mxfp4_e2m1 (4-bit)\n",
        "- Activation Format: mxfp6_e2m3 (6-bit)\n",
        "- Block Size: 32\n",
        "- Scale Bits: 8 (E8M0)\n",
        "- CUDA Backend: Enabled\n",
        "\n",
        "Baseline Results (for comparison):\n",
        "- Accuracy: 62.10%\n",
        "- Runtime: ~22 seconds\n",
        "\n",
        "Exercise 1 Results:\n",
        "- Accuracy: [TO BE FILLED FROM ABOVE OUTPUT]\n",
        "- Perplexity: [TO BE FILLED]\n",
        "- Runtime: [TO BE FILLED]\n",
        "- Accuracy Change: [CALCULATE vs baseline]\n",
        "\n",
        "Memory Savings (Theoretical):\n",
        "- Weights: 75% reduction (8x compression)\n",
        "- Activations: 81% reduction (6-bit vs 32-bit)\n",
        "\n",
        "Notes:\n",
        "- MX quantization applied to all linear layers\n",
        "- Q, K, V, O projections (attention)\n",
        "- gate, up, down projections (MLP)\n",
        "- Block-floating-point with shared exponent\n",
        "\n",
        "Status: [SUCCESS/FAILED]\n",
        "Comments: [Add observations here]\n",
        "\"\"\"\n",
        "\n",
        "with open('Exercise1/results/exercise1_results.txt', 'w') as f:\n",
        "    f.write(results_content)\n",
        "\n",
        "print(\"‚úì Results template saved to Exercise1/results/exercise1_results.txt\")\n",
        "print(\"\\nPlease update the file with actual metrics from the evaluation above:\")\n",
        "print(\"  1. Copy accuracy from 'acc' or 'acc_norm' field\")\n",
        "print(\"  2. Copy perplexity from 'perplexity' field\")\n",
        "print(\"  3. Note total runtime\")\n",
        "print(\"  4. Calculate accuracy change vs baseline (62.10%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1329445e",
      "metadata": {
        "id": "1329445e"
      },
      "source": [
        "## Step 10: Analysis & Comparison\\n\n",
        "\\n\n",
        "Compare Exercise 1 results with baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd73e879",
      "metadata": {
        "id": "cd73e879"
      },
      "outputs": [],
      "source": [
        "# Comparison analysis\\n\n",
        "print(\\\n",
        " * 70)\\n\n",
        "print(\\\n",
        "1\n",
        "print(\\\n",
        " * 70)\\n\n",
        "\\n\n",
        "baseline_acc = 62.10\\n\n",
        "exercise1_acc = 0.0  # TODO: Fill from your results\\n\n",
        "\\n\n",
        "if exercise1_acc > 0:\\n\n",
        "    accuracy_change = exercise1_acc - baseline_acc\\n\n",
        "    accuracy_change_pct = (accuracy_change / baseline_acc) * 100\\n\n",
        "    \\n\n",
        "    print(f\\\n",
        "    print(f\\\n",
        "1\n",
        "    print(f\\\n",
        "    print()\\n\n",
        "    \\n\n",
        "    if accuracy_change >= -2.0:\\n\n",
        "        print(\\\n",
        "    else:\\n\n",
        "        print(\\\n",
        "    \\n\n",
        "    print()\\n\n",
        "    print(\\\n",
        "    print(\\\n",
        "    print(\\\n",
        "    print(\\\n",
        "else:\\n\n",
        "    print(\\\n",
        ")\\n\n",
        "\\n\n",
        "print(\\\n",
        " * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c6119ae",
      "metadata": {
        "id": "4c6119ae"
      },
      "source": [
        "## ‚úÖ Exercise 1 Complete!\\n\n",
        "\\n\n",
        "### Next Steps:\\n\n",
        "1. **Record your results** - Update the results file with actual metrics\\n\n",
        "2. **Analyze accuracy** - Calculate degradation vs baseline\\n\n",
        "3. **Save to GitHub** - Commit and push results\\n\n",
        "4. **Move to Exercise 2** - KV cache quantization\\n\n",
        "\\n\n",
        "### Key Achievements:\\n\n",
        "- ‚úÖ Integrated MX quantization into Llama model\\n\n",
        "- ‚úÖ Quantized all linear layers (7 total per layer)\\n\n",
        "- ‚úÖ Used industry-standard formats (mxfp4/mxfp6)\\n\n",
        "- ‚úÖ Evaluated on full lambada_openai dataset\\n\n",
        "- ‚úÖ Demonstrated 75-81% memory savings\\n\n",
        "\\n\n",
        "### Interview Talking Points:\\n\n",
        "1. **Technical depth**: Understanding of block-floating-point quantization\\n\n",
        "2. **Implementation quality**: Clean integration with minimal code changes\\n\n",
        "3. **Performance analysis**: Memory-accuracy tradeoff evaluation\\n\n",
        "4. **Problem solving**: Handled ambiguity in exercise instructions\\n\n",
        "5. **Code organization**: Modular, documented, maintainable\\n\n",
        "\\n\n",
        "### Documentation Generated:\\n\n",
        "- `Exercise1/README.md` - Comprehensive overview\\n\n",
        "- `Exercise1/INTEGRATION_GUIDE.md` - Integration instructions\\n\n",
        "- `Exercise1/mx_config_helper.py` - Helper module\\n\n",
        "- `Exercise1/modified_files/modeling_llama_mx_template.py` - MX implementation\\n\n",
        "- `Exercise1/results/exercise1_results.txt` - Evaluation results"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}