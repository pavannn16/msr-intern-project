# Microsoft Research Internship - AI/ML Numerics & Efficiency

## Project Overview
This repository contains work for the Microsoft Research Internship take-home assignment focusing on model quantization using Microxcaling (MX) data formats on the Llama-3.2-1B model.

## ğŸ¯ Objectives
1. **Exercise 1**: Quantize linear layers with MX (weights: mxfp4_e2m1, activations: mxfp6_e2m3)
2. **Exercise 2**: Quantize KV cache with MX (mxfp4_e2m1)
3. **Exercise 3** (Optional): Implement E5M3 scale factor support

## ğŸ“ Repository Structure
```
msr-intern-project/
â”œâ”€â”€ internexercise.txt          # Exercise instructions
â”œâ”€â”€ internshipmail.txt          # Communication with Microsoft
â”œâ”€â”€ scripts/                    # Setup and utility scripts
â”‚   â””â”€â”€ setup_colab.sh         # Colab environment setup
â”œâ”€â”€ modified_files/             # Modified transformers files
â”‚   â””â”€â”€ modeling_llama.py      # MX-integrated Llama model
â”œâ”€â”€ results/                    # Evaluation results
â”‚   â”œâ”€â”€ baseline_results.txt
â”‚   â”œâ”€â”€ exercise1_results.txt
â”‚   â””â”€â”€ exercise2_results.txt    # (pending)
â””â”€â”€ report/                     # Final report and analysis
    â””â”€â”€ technical_report.md
```

## ğŸš€ Setup Instructions

### Prerequisites
- Google Colab account with GPU access (T4/A100/H100)
- Hugging Face account with Llama-3.2-1B access
- HF access token

### Setup in Google Colab
1. Clone this repository:
```bash
git clone https://github.com/pavannn16/msr-intern-project.git
cd msr-intern-project
```

2. Run the setup script:
```bash
bash scripts/setup_colab.sh
```

3. Set your HF token:
```bash
export HF_TOKEN=<your_token_here>
```

## ğŸ“Š Timeline
- **Exercises 1 & 2 Due**: February 8, 2026, 5 PM PST
- **Exercise 3 Due**: February 13, 2026
- **Interview Window**: February 9-14, 2026
- **Baseline Completed**: January 29, 2026 âœ… (62.10% accuracy)
- **Exercise 1 Completed**: January 29, 2026 âœ… (Implementation ready)

## ğŸ” Security & NDA
This repository is **PRIVATE** to comply with Microsoft NDA requirements. All work is original and properly attributed.

## ğŸ“ Progress Tracking
- [x] Environment setup
- [x] Baseline evaluation (62.10% accuracy) âœ…
- [x] Exercise 1: Linear layer quantization (implementation complete) âœ…
- [x] Exercise 1: Evaluation & results (currently below target accuracy)
- [ ] Exercise 2: KV cache quantization
- [ ] Exercise 3: E5M3 scale factor (optional)
- [ ] Technical report

## ğŸ› ï¸ Key Technologies
- **Model**: meta-llama/Llama-3.2-1B
- **Framework**: PyTorch, Transformers v4.57.6
- **Quantization**: Microsoft Microxcaling (MX)
- **Evaluation**: lm-eval harness (lambada_openai)
- **Platform**: Google Colab with GPU

## ğŸ“§ Contact
Pavan Chauhan - pavanc1604@gmail.com

---
*Last Updated: January 31, 2026*
