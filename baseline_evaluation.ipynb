{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00c2ed48",
   "metadata": {
    "id": "00c2ed48"
   },
   "source": [
    "# MSR Internship - Baseline Evaluation\n",
    "## Llama-3.2-1B on lambada_openai Task\n",
    "\n",
    "This notebook runs the baseline evaluation for the Microsoft Research Internship exercise.\n",
    "\n",
    "**Prerequisites:**\n",
    "- ‚úÖ Google Colab with GPU runtime (T4/A100/H100)\n",
    "- ‚úÖ Hugging Face account with Llama-3.2-1B access\n",
    "- ‚úÖ HF access token\n",
    "\n",
    "**Expected Baseline Result:** ~62.24% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab5464",
   "metadata": {
    "id": "17ab5464"
   },
   "source": [
    "## Step 1: Verify GPU Runtime\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT:** Before running any cells, make sure you're using a GPU runtime:\n",
    "1. Click the dropdown in top-right corner (near RAM/Disk)\n",
    "2. Select \"Change runtime type\"\n",
    "3. Choose **T4 GPU** (or A100/H100 if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1be8c3ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1be8c3ce",
    "outputId": "fe16ef3e-92f6-49f0-9b28-749aee5d6c47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 29 22:58:36 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   31C    P0             45W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f442317f",
   "metadata": {
    "id": "f442317f"
   },
   "source": [
    "## Step 2: Clone Project Repository\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT:** Since this is a private repository, you need a GitHub Personal Access Token.\n",
    "\n",
    "### Create GitHub Token:\n",
    "1. Go to: https://github.com/settings/tokens\n",
    "2. Click \"Generate new token\" ‚Üí \"Generate new token (classic)\"\n",
    "3. Give it a name (e.g., \"Colab Access\")\n",
    "4. Select scope: ‚úÖ **repo** (Full control of private repositories)\n",
    "5. Click \"Generate token\"\n",
    "6. **Copy the token immediately** (shown only once)\n",
    "7. Replace `'your_github_token_here'` in **cell 5 below** with your actual token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61ed9e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c61ed9e0",
    "outputId": "1aafd632-1d61-4995-950d-10857393a2a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'msr-intern-project'...\n",
      "fatal: could not read Username for 'https://github.com': No such device or address\n",
      "[Errno 2] No such file or directory: 'msr-intern-project'\n",
      "/content\n"
     ]
    }
   ],
   "source": [
    "# Clone the project repository (private repo - requires authentication)\n",
    "GITHUB_TOKEN = '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'\n",
    "\n",
    "!git clone https://{GITHUB_TOKEN}@github.com/pavannn16/msr-intern-project.git\n",
    "%cd msr-intern-project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853f85b8",
   "metadata": {
    "id": "853f85b8"
   },
   "source": [
    "## Step 3: Run Automated Setup\n",
    "\n",
    "This will:\n",
    "- Clone transformers (v4.57.6) and microxcaling repositories\n",
    "- Install lm-eval and dependencies\n",
    "- Set up environment variables\n",
    "\n",
    "‚è±Ô∏è **Estimated time:** 3-5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6a12922",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6a12922",
    "outputId": "15bfdbca-fcd7-4cfd-e96a-4e24574d6391"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash: scripts/setup_colab.sh: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Run the automated setup script\n",
    "!bash scripts/setup_colab.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0f0d4c",
   "metadata": {
    "id": "3f0f0d4c"
   },
   "source": [
    "## Step 4: Set Hugging Face Token\n",
    "\n",
    "‚ö†Ô∏è **REPLACE `your_token_here` WITH YOUR ACTUAL HF TOKEN**\n",
    "\n",
    "Get your token from: https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc8e5021",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc8e5021",
    "outputId": "c3cdcd5c-80d9-4696-e378-f069e7fa490e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì HF_TOKEN set successfully\n"
     ]
    }
   ],
   "source": [
    "# Set your Hugging Face token\n",
    "import os\n",
    "os.environ['HF_TOKEN'] = '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'\n",
    "\n",
    "# Verify token is set\n",
    "print(\"‚úì HF_TOKEN set successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e07729",
   "metadata": {
    "id": "d7e07729"
   },
   "source": [
    "## Step 5: Quick Test (10% of Dataset)\n",
    "\n",
    "Test the setup with a small subset to verify everything works.\n",
    "\n",
    "‚è±Ô∏è **Estimated time:** 1-2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b989bcd7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b989bcd7",
    "outputId": "a1cb2f6a-f37a-40cd-c37f-02c544a47aa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: lm_eval: command not found\n"
     ]
    }
   ],
   "source": [
    "# Quick test with 10% of the dataset\n",
    "!lm_eval --model hf \\\n",
    "  --model_args pretrained=meta-llama/Llama-3.2-1B \\\n",
    "  --tasks lambada_openai \\\n",
    "  --device cuda \\\n",
    "  --batch_size 32 \\\n",
    "  --limit 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084fb713",
   "metadata": {
    "id": "084fb713"
   },
   "source": [
    "## Step 6: Full Baseline Evaluation\n",
    "\n",
    "Run the complete evaluation on 100% of the lambada_openai dataset.\n",
    "\n",
    "‚è±Ô∏è **Estimated time:** 10-15 minutes  \n",
    "üéØ **Expected accuracy:** ~62.24%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "160f5e29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "160f5e29",
    "outputId": "ef770349-c749-4851-869e-705f1e0ee560"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: lm_eval: command not found\n"
     ]
    }
   ],
   "source": [
    "# Full baseline evaluation\n",
    "!lm_eval --model hf \\\n",
    "  --model_args pretrained=meta-llama/Llama-3.2-1B \\\n",
    "  --tasks lambada_openai \\\n",
    "  --device cuda \\\n",
    "  --batch_size 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780d2ebd",
   "metadata": {
    "id": "780d2ebd"
   },
   "source": [
    "## Step 7: Save Results\n",
    "\n",
    "Save the baseline results for comparison with quantized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30251cf0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "30251cf0",
    "outputId": "8285bb97-d50c-4c82-a022-a7319fe7420f"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/baseline_results.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1310045695.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtimestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y-%m-%d %H:%M:%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results/baseline_results.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Baseline Evaluation Results\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/baseline_results.txt'"
     ]
    }
   ],
   "source": [
    "# Save baseline results to file\n",
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "with open('results/baseline_results.txt', 'w') as f:\n",
    "    f.write(f\"Baseline Evaluation Results\\n\")\n",
    "    f.write(f\"=\" * 50 + \"\\n\")\n",
    "    f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "    f.write(f\"Model: meta-llama/Llama-3.2-1B\\n\")\n",
    "    f.write(f\"Task: lambada_openai\\n\")\n",
    "    f.write(f\"Device: CUDA\\n\")\n",
    "    f.write(f\"Batch Size: 32\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"Results:\\n\")\n",
    "    f.write(f\"Accuracy: [TO BE FILLED FROM ABOVE OUTPUT]\\n\")\n",
    "    f.write(f\"Runtime: [TO BE FILLED FROM ABOVE OUTPUT]\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"NOTE: Manually update this file with the actual results from the evaluation output above.\\n\")\n",
    "\n",
    "print(\"‚úì Results template saved to results/baseline_results.txt\")\n",
    "print(\"\\n‚ö†Ô∏è Remember to:\")\n",
    "print(\"  1. Copy the actual accuracy and runtime from the output above\")\n",
    "print(\"  2. Update the results file\")\n",
    "print(\"  3. Commit and push results to GitHub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56c41e",
   "metadata": {
    "id": "aa56c41e"
   },
   "source": [
    "## ‚úÖ Baseline Evaluation Complete!\n",
    "\n",
    "### Next Steps:\n",
    "1. **Record your results** - Note the accuracy and runtime from the evaluation above\n",
    "2. **Compare with expected** - Should be around 62.24% accuracy\n",
    "3. **Save to GitHub** - If running in Colab, download the results file\n",
    "4. **Move to Exercise 1** - Start implementing MX quantization for linear layers\n",
    "\n",
    "### Troubleshooting:\n",
    "- **HF Token Error**: Verify token has Read permission and Llama-3.2-1B access approved\n",
    "- **CUDA Error**: Check GPU runtime is enabled (T4/A100/H100)\n",
    "- **Out of Memory**: Reduce batch_size to 16 or 8\n",
    "- **Import Errors**: Re-run the setup script\n",
    "\n",
    "### Important Files:\n",
    "- `results/baseline_results.txt` - Your baseline metrics\n",
    "- `modified_files/` - Where Exercise 1 modifications will go\n",
    "- `scripts/` - Helper scripts for setup and evaluation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
