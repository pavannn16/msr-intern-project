{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00c2ed48",
   "metadata": {},
   "source": [
    "# MSR Internship - Baseline Evaluation\n",
    "## Llama-3.2-1B on lambada_openai Task\n",
    "\n",
    "This notebook runs the baseline evaluation for the Microsoft Research Internship exercise.\n",
    "\n",
    "**Prerequisites:**\n",
    "- ‚úÖ Google Colab with GPU runtime (T4/A100/H100)\n",
    "- ‚úÖ Hugging Face account with Llama-3.2-1B access\n",
    "- ‚úÖ HF access token\n",
    "\n",
    "**Expected Baseline Result:** ~62.24% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab5464",
   "metadata": {},
   "source": [
    "## Step 1: Verify GPU Runtime\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT:** Before running any cells, make sure you're using a GPU runtime:\n",
    "1. Click the dropdown in top-right corner (near RAM/Disk)\n",
    "2. Select \"Change runtime type\"\n",
    "3. Choose **T4 GPU** (or A100/H100 if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be8c3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f442317f",
   "metadata": {},
   "source": [
    "## Step 2: Clone Project Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61ed9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the project repository\n",
    "!git clone https://github.com/pavannn16/msr-intern-project.git\n",
    "%cd msr-intern-project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853f85b8",
   "metadata": {},
   "source": [
    "## Step 3: Run Automated Setup\n",
    "\n",
    "This will:\n",
    "- Clone transformers (v4.57.6) and microxcaling repositories\n",
    "- Install lm-eval and dependencies\n",
    "- Set up environment variables\n",
    "\n",
    "‚è±Ô∏è **Estimated time:** 3-5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a12922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the automated setup script\n",
    "!bash scripts/setup_colab.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0f0d4c",
   "metadata": {},
   "source": [
    "## Step 4: Set Hugging Face Token\n",
    "\n",
    "‚ö†Ô∏è **REPLACE `your_token_here` WITH YOUR ACTUAL HF TOKEN**\n",
    "\n",
    "Get your token from: https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8e5021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Hugging Face token\n",
    "import os\n",
    "os.environ['HF_TOKEN'] = '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'\n",
    "\n",
    "# Verify token is set\n",
    "print(\"‚úì HF_TOKEN set successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e07729",
   "metadata": {},
   "source": [
    "## Step 5: Quick Test (10% of Dataset)\n",
    "\n",
    "Test the setup with a small subset to verify everything works.\n",
    "\n",
    "‚è±Ô∏è **Estimated time:** 1-2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b989bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with 10% of the dataset\n",
    "!lm_eval --model hf \\\n",
    "  --model_args pretrained=meta-llama/Llama-3.2-1B \\\n",
    "  --tasks lambada_openai \\\n",
    "  --device cuda \\\n",
    "  --batch_size 32 \\\n",
    "  --limit 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084fb713",
   "metadata": {},
   "source": [
    "## Step 6: Full Baseline Evaluation\n",
    "\n",
    "Run the complete evaluation on 100% of the lambada_openai dataset.\n",
    "\n",
    "‚è±Ô∏è **Estimated time:** 10-15 minutes  \n",
    "üéØ **Expected accuracy:** ~62.24%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full baseline evaluation\n",
    "!lm_eval --model hf \\\n",
    "  --model_args pretrained=meta-llama/Llama-3.2-1B \\\n",
    "  --tasks lambada_openai \\\n",
    "  --device cuda \\\n",
    "  --batch_size 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780d2ebd",
   "metadata": {},
   "source": [
    "## Step 7: Save Results\n",
    "\n",
    "Save the baseline results for comparison with quantized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30251cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save baseline results to file\n",
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "with open('results/baseline_results.txt', 'w') as f:\n",
    "    f.write(f\"Baseline Evaluation Results\\n\")\n",
    "    f.write(f\"=\" * 50 + \"\\n\")\n",
    "    f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "    f.write(f\"Model: meta-llama/Llama-3.2-1B\\n\")\n",
    "    f.write(f\"Task: lambada_openai\\n\")\n",
    "    f.write(f\"Device: CUDA\\n\")\n",
    "    f.write(f\"Batch Size: 32\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"Results:\\n\")\n",
    "    f.write(f\"Accuracy: [TO BE FILLED FROM ABOVE OUTPUT]\\n\")\n",
    "    f.write(f\"Runtime: [TO BE FILLED FROM ABOVE OUTPUT]\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"NOTE: Manually update this file with the actual results from the evaluation output above.\\n\")\n",
    "\n",
    "print(\"‚úì Results template saved to results/baseline_results.txt\")\n",
    "print(\"\\n‚ö†Ô∏è Remember to:\")\n",
    "print(\"  1. Copy the actual accuracy and runtime from the output above\")\n",
    "print(\"  2. Update the results file\")\n",
    "print(\"  3. Commit and push results to GitHub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56c41e",
   "metadata": {},
   "source": [
    "## ‚úÖ Baseline Evaluation Complete!\n",
    "\n",
    "### Next Steps:\n",
    "1. **Record your results** - Note the accuracy and runtime from the evaluation above\n",
    "2. **Compare with expected** - Should be around 62.24% accuracy\n",
    "3. **Save to GitHub** - If running in Colab, download the results file\n",
    "4. **Move to Exercise 1** - Start implementing MX quantization for linear layers\n",
    "\n",
    "### Troubleshooting:\n",
    "- **HF Token Error**: Verify token has Read permission and Llama-3.2-1B access approved\n",
    "- **CUDA Error**: Check GPU runtime is enabled (T4/A100/H100)\n",
    "- **Out of Memory**: Reduce batch_size to 16 or 8\n",
    "- **Import Errors**: Re-run the setup script\n",
    "\n",
    "### Important Files:\n",
    "- `results/baseline_results.txt` - Your baseline metrics\n",
    "- `modified_files/` - Where Exercise 1 modifications will go\n",
    "- `scripts/` - Helper scripts for setup and evaluation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
