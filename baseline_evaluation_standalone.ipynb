{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d197686",
   "metadata": {},
   "source": [
    "# MSR Internship - Baseline Evaluation (Standalone)\n",
    "## Llama-3.2-1B on lambada_openai Task\n",
    "\n",
    "This notebook runs the baseline evaluation **without requiring GitHub authentication**.\n",
    "\n",
    "**Prerequisites:**\n",
    "- ‚úÖ Google Colab with GPU runtime (T4/A100/H100)\n",
    "- ‚úÖ Hugging Face account with Llama-3.2-1B access\n",
    "- ‚úÖ HF access token\n",
    "\n",
    "**Expected Baseline Result:** ~62.24% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0499d17c",
   "metadata": {},
   "source": [
    "## Step 1: Verify GPU Runtime\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT:** Before running any cells, make sure you're using a GPU runtime:\n",
    "1. Click the dropdown in top-right corner (near RAM/Disk)\n",
    "2. Select \"Change runtime type\"\n",
    "3. Choose **T4 GPU** (or A100/H100 if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26239280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208b40d6",
   "metadata": {},
   "source": [
    "## Step 2: Setup Environment\n",
    "\n",
    "‚è±Ô∏è **Estimated time:** 3-5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9010096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone transformers and microxcaling repositories\n",
    "!git clone https://github.com/huggingface/transformers.git /content/transformers\n",
    "!git clone https://github.com/microsoft/microxcaling.git /content/microxcaling\n",
    "\n",
    "# Checkout specific transformers version\n",
    "%cd /content/transformers\n",
    "!git checkout v4.57.6\n",
    "\n",
    "# Install transformers\n",
    "!pip install -e . -q\n",
    "\n",
    "# Install lm-eval and dependencies\n",
    "!pip install lm_eval ninja -q\n",
    "\n",
    "print(\"\\n‚úì Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e820d9b5",
   "metadata": {},
   "source": [
    "## Step 3: Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7330427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Python path and HF token\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add microxcaling to Python path\n",
    "sys.path.insert(0, '/content/microxcaling')\n",
    "os.environ['PYTHONPATH'] = '/content/microxcaling'\n",
    "\n",
    "# Set your Hugging Face token\n",
    "os.environ['HF_TOKEN'] = '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'\n",
    "\n",
    "print(\"‚úì Environment variables set successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3629492b",
   "metadata": {},
   "source": [
    "## Step 4: Quick Test (10% of Dataset)\n",
    "\n",
    "Test the setup with a small subset to verify everything works.\n",
    "\n",
    "‚è±Ô∏è **Estimated time:** 1-2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d0f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with 10% of the dataset\n",
    "!lm_eval --model hf \\\n",
    "  --model_args pretrained=meta-llama/Llama-3.2-1B \\\n",
    "  --tasks lambada_openai \\\n",
    "  --device cuda \\\n",
    "  --batch_size 32 \\\n",
    "  --limit 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0fe5cd",
   "metadata": {},
   "source": [
    "## Step 5: Full Baseline Evaluation\n",
    "\n",
    "Run the complete evaluation on 100% of the lambada_openai dataset.\n",
    "\n",
    "‚è±Ô∏è **Estimated time:** 10-15 minutes  \n",
    "üéØ **Expected accuracy:** ~62.24%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5721ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full baseline evaluation\n",
    "!lm_eval --model hf \\\n",
    "  --model_args pretrained=meta-llama/Llama-3.2-1B \\\n",
    "  --tasks lambada_openai \\\n",
    "  --device cuda \\\n",
    "  --batch_size 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d16dd8",
   "metadata": {},
   "source": [
    "## Step 6: Create Results Directory and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185da9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "!mkdir -p /content/results\n",
    "\n",
    "# Save baseline results to file\n",
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "with open('/content/results/baseline_results.txt', 'w') as f:\n",
    "    f.write(f\"Baseline Evaluation Results\\n\")\n",
    "    f.write(f\"=\" * 50 + \"\\n\")\n",
    "    f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "    f.write(f\"Model: meta-llama/Llama-3.2-1B\\n\")\n",
    "    f.write(f\"Task: lambada_openai\\n\")\n",
    "    f.write(f\"Device: CUDA\\n\")\n",
    "    f.write(f\"Batch Size: 32\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"Results:\\n\")\n",
    "    f.write(f\"Accuracy: [TO BE FILLED FROM ABOVE OUTPUT]\\n\")\n",
    "    f.write(f\"Runtime: [TO BE FILLED FROM ABOVE OUTPUT]\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"NOTE: Manually update this file with the actual results from the evaluation output above.\\n\")\n",
    "\n",
    "print(\"‚úì Results template saved to /content/results/baseline_results.txt\")\n",
    "print(\"\\n‚ö†Ô∏è Remember to:\")\n",
    "print(\"  1. Copy the actual accuracy and runtime from the output above\")\n",
    "print(\"  2. Update the results file\")\n",
    "print(\"  3. Download the file from Colab (Files panel on left)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10cec34",
   "metadata": {},
   "source": [
    "## ‚úÖ Baseline Evaluation Complete!\n",
    "\n",
    "### Next Steps:\n",
    "1. **Record your results** - Note the accuracy and runtime from Step 5\n",
    "2. **Compare with expected** - Should be around 62.24% accuracy\n",
    "3. **Download results** - Use Files panel on left to download results file\n",
    "4. **Move to Exercise 1** - Start implementing MX quantization for linear layers\n",
    "\n",
    "### Troubleshooting:\n",
    "- **HF Token Error**: Verify token has Read permission and Llama-3.2-1B access approved\n",
    "- **CUDA Error**: Check GPU runtime is enabled (T4/A100/H100)\n",
    "- **Out of Memory**: Reduce batch_size to 16 or 8\n",
    "- **Import Errors**: Re-run Step 2 (setup)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
