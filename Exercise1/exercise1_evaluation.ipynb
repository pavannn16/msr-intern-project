{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "638e2d14",
   "metadata": {
    "id": "638e2d14"
   },
   "source": [
    "# Exercise 1: MX Quantization of Linear Layers\n",
    "\n",
    "## Llama-3.2-1B with mxfp4_e2m1 (weights) + mxfp6_e2m3 (activations)\n",
    "\n",
    "This notebook evaluates an MX-quantized Llama model on the `lambada_openai` task.\n",
    "\n",
    "**Exercise objectives**\n",
    "- Quantize all linear layers (Q, K, V, O, gate, up, down)\n",
    "- Use `mxfp4_e2m1` for weights (4-bit)\n",
    "- Use `mxfp6_e2m3` for activations (6-bit)\n",
    "- Compare accuracy vs baseline (62.10%)\n",
    "\n",
    "**Expected outcome**\n",
    "- Accuracy target: > 60% (< 2% degradation)\n",
    "\n",
    "**Author:** Pavan Chauhan  \n",
    "**Date:** January 30, 2026\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is designed to run top-to-bottom without manual restarts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270619e7",
   "metadata": {
    "id": "270619e7"
   },
   "source": [
    "## Step 1: Verify GPU Runtime\n",
    "\n",
    "Ensure GPU runtime is enabled (T4/A100/H100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4b91c23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4b91c23",
    "outputId": "bc29570a-093c-43a2-8a7a-9c19092881fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan 31 14:20:15 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   30C    P0             42W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488c0506",
   "metadata": {
    "id": "488c0506"
   },
   "source": [
    "## Step 2: Clone Project Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c04e39c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0c04e39c",
    "outputId": "bda364e2-fb74-4e2b-ff09-db999671afce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/msr-intern-project\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clone / update the project repository\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "repo_dir = \"/content/msr-intern-project\"\n",
    "if not os.path.exists(repo_dir):\n",
    "    subprocess.check_call([\"git\", \"clone\", \"https://github.com/pavannn16/msr-intern-project.git\", repo_dir])\n",
    "else:\n",
    "    subprocess.check_call([\"git\", \"-C\", repo_dir, \"fetch\", \"origin\", \"main\"])\n",
    "    subprocess.check_call([\"git\", \"-C\", repo_dir, \"reset\", \"--hard\", \"origin/main\"])\n",
    "\n",
    "%cd /content/msr-intern-project\n",
    "subprocess.check_call([\"git\", \"rev-parse\", \"--short\", \"HEAD\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c00750b",
   "metadata": {
    "id": "8c00750b"
   },
   "source": [
    "## Step 3: Install Dependencies\n",
    "\n",
    "Install `transformers`, `microxcaling` (MX), and `lm-eval`.\n",
    "\n",
    "Estimated time: 2-3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d67f3c57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d67f3c57",
    "outputId": "c65a55cc-2310-4d34-c793-11c6068269a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies...\n",
      "+ /usr/bin/python3 -m pip install -q transformers==4.57.6 lm_eval ninja\n",
      "torch=2.9.0+cu126, cuda=12.6, device_count=1\n",
      "microxcaling cloned\n",
      "+ /usr/bin/python3 -m pip install -q -e /content/microxcaling --no-deps\n",
      "MX import: OK\n",
      "All dependencies installed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"Installing dependencies...\")\n",
    "\n",
    "def pip_install(args):\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + args\n",
    "    print(\"+\", \" \".join(cmd))\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "# Core packages (do not force-install torch; Colab typically provides a working CUDA build)\n",
    "pip_install([\"transformers==4.57.6\", \"lm_eval\", \"ninja\"])\n",
    "\n",
    "# Confirm torch is available and report version\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"torch={torch.__version__}, cuda={torch.version.cuda}, device_count={torch.cuda.device_count()}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"PyTorch is not available in this runtime: {e}\")\n",
    "\n",
    "# Clone microxcaling (MX)\n",
    "mx_repo_dir = \"/content/microxcaling\"\n",
    "if not os.path.exists(mx_repo_dir):\n",
    "    subprocess.check_call([\"git\", \"clone\", \"-q\", \"https://github.com/microsoft/microxcaling.git\", mx_repo_dir])\n",
    "    print(\"microxcaling cloned\")\n",
    "else:\n",
    "    print(\"microxcaling already exists\")\n",
    "\n",
    "# Install microxcaling WITHOUT deps to avoid torchaudio pinning issues.\n",
    "# The Python package is commonly located under a subdirectory (e.g., /python).\n",
    "mx_python_root = os.path.join(mx_repo_dir, \"python\") if os.path.isdir(os.path.join(mx_repo_dir, \"python\")) else mx_repo_dir\n",
    "pip_install([\"-e\", mx_python_root, \"--no-deps\"])\n",
    "\n",
    "# Sanity check: MX import (fallback to sys.path if editable install layout differs)\n",
    "try:\n",
    "    import mx  # noqa: F401\n",
    "except ModuleNotFoundError:\n",
    "    if mx_python_root not in sys.path:\n",
    "        sys.path.insert(0, mx_python_root)\n",
    "    import mx  # noqa: F401\n",
    "\n",
    "print(\"MX import: OK\")\n",
    "print(\"All dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f3f3ef",
   "metadata": {
    "id": "64f3f3ef"
   },
   "source": [
    "## Step 4: Setup Exercise 1 Files\n",
    "\n",
    "Copy the complete MX-integrated modeling_llama.py to transformers package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47501700",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47501700",
    "outputId": "a11967fa-e715-414b-9138-e94be4d4f9e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Exercise 1 files...\n",
      "Transformers installed at: /usr/local/lib/python3.12/dist-packages/transformers\n",
      "Found MX-integrated file (725 lines)\n",
      "Backup created\n",
      "MX-integrated modeling_llama.py deployed\n",
      "MX quantization functions detected in deployed file\n",
      "\n",
      "Exercise 1 setup complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import transformers\n",
    "\n",
    "print(\"Setting up Exercise 1 files...\")\n",
    "\n",
    "transformers_path = os.path.dirname(transformers.__file__)\n",
    "print(f\"Transformers installed at: {transformers_path}\")\n",
    "\n",
    "modeling_llama_path = os.path.join(transformers_path, \"models\", \"llama\", \"modeling_llama.py\")\n",
    "backup_path = modeling_llama_path + \".backup\"\n",
    "\n",
    "mx_complete_file = \"/content/msr-intern-project/Exercise1/modified_files/modeling_llama.py\"\n",
    "if os.path.exists(mx_complete_file):\n",
    "    with open(mx_complete_file, \"r\") as f:\n",
    "        line_count = len(f.readlines())\n",
    "    print(f\"Found MX-integrated file ({line_count} lines)\")\n",
    "\n",
    "    if not os.path.exists(backup_path):\n",
    "        shutil.copy2(modeling_llama_path, backup_path)\n",
    "        print(\"Backup created\")\n",
    "\n",
    "    shutil.copy2(mx_complete_file, modeling_llama_path)\n",
    "    print(\"MX-integrated modeling_llama.py deployed\")\n",
    "\n",
    "    with open(modeling_llama_path, \"r\") as f:\n",
    "        content = f.read()\n",
    "    if \"apply_mx_linear\" in content:\n",
    "        print(\"MX quantization functions detected in deployed file\")\n",
    "    else:\n",
    "        print(\"Warning: MX functions not found in deployed file\")\n",
    "else:\n",
    "    print(f\"Warning: MX file not found at {mx_complete_file}\")\n",
    "    print(\"  Evaluation will use standard transformers (no MX quantization)\")\n",
    "\n",
    "print(\"\\nExercise 1 setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8839dd3a",
   "metadata": {
    "id": "8839dd3a"
   },
   "source": [
    "## Step 5: Set Hugging Face Token\n",
    "\n",
    "Set your Hugging Face token for model access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9b2fa9",
   "metadata": {
    "id": "af9b2fa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF token is set (value hidden)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Option A (recommended on Colab): use Secrets\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
    "\n",
    "# Option B: manually set the token in this runtime (do not commit real tokens)\n",
    "os.environ[\"HF_TOKEN\"] = \"YOUR_HF_TOKEN_HERE\"\n",
    "\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if not hf_token or hf_token == \"YOUR_HF_TOKEN_HERE\":\n",
    "    raise RuntimeError(\n",
    "        \"HF_TOKEN is not set. Set it via Colab Secrets (recommended) or set the token in this cell for runtime-only use (do not commit real tokens).\"\n",
    "    )\n",
    "\n",
    "print(\"HF token is set (value hidden)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56d8ed7",
   "metadata": {
    "id": "b56d8ed7"
   },
   "source": [
    "## Step 6: Verify MX Integration\n",
    "\n",
    "Test that MX library and modified model load correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2002d63b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "2002d63b",
    "outputId": "ac3da440-bdf8-47a1-9595-55ef4a90d62d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying MX integration...\n",
      "MX library import: OK\n",
      "MX Quantization Configuration:\n",
      "==================================================\n",
      "Weights: fp4_e2m1 (4-bit)\n",
      "Activations: fp6_e2m3 (6-bit)\n",
      "Scale Bits: 8 (E8M0)\n",
      "Block Size: 32\n",
      "CUDA Backend: Enabled\n",
      "Rounding: even\n",
      "Backward Quantization: Disabled\n",
      "==================================================\n",
      "Transformers llama module: /usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\n",
      "Deployed MX rounding mode: even\n",
      "Trial 2 semantics detected: True\n",
      "MX integration detected in LlamaMLP.forward.\n",
      "MX integration verification complete\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import inspect\n",
    "import sys\n",
    "\n",
    "print(\"Verifying MX integration...\")\n",
    "\n",
    "exercise1_dir = \"/content/msr-intern-project/Exercise1\"\n",
    "if exercise1_dir not in sys.path:\n",
    "    sys.path.insert(0, exercise1_dir)\n",
    "\n",
    "# 1) MX import\n",
    "try:\n",
    "    from mx import linear as mx_linear  # noqa: F401\n",
    "    from mx.specs import MxSpecs  # noqa: F401\n",
    "    print(\"MX library import: OK\")\n",
    "except ImportError as e:\n",
    "    raise ImportError(\n",
    "        f\"MX library import failed: {e}. Ensure Step 3 installed microxcaling (pip install -e /content/microxcaling).\"\n",
    "    )\n",
    "\n",
    "# 2) Local helper import\n",
    "try:\n",
    "    from mx_config_helper import create_mx_specs_exercise1, print_mx_specs_summary\n",
    "    mx_specs = create_mx_specs_exercise1()\n",
    "    print_mx_specs_summary(mx_specs)\n",
    "except ImportError as e:\n",
    "    raise ImportError(\n",
    "        f\"Failed to import Exercise 1 helper(s): {e}. Ensure the repo is cloned and Step 2 ran successfully.\"\n",
    "    )\n",
    "\n",
    "# 3) Confirm the deployed transformers Llama file is the MX-integrated one\n",
    "import transformers\n",
    "\n",
    "llama_mod = transformers.models.llama.modeling_llama\n",
    "llama_file = getattr(llama_mod, \"__file__\", \"<unknown>\")\n",
    "print(f\"Transformers llama module: {llama_file}\")\n",
    "\n",
    "with open(llama_file, \"r\") as f:\n",
    "    deployed_src = f.read()\n",
    "\n",
    "if \"apply_mx_linear\" not in deployed_src:\n",
    "    raise RuntimeError(\n",
    "        \"Deployed transformers modeling_llama.py does not appear to include MX integration. Re-run Step 4 (deploy).\"\n",
    "    )\n",
    "\n",
    "# Extra: confirm the deployed MX specs match expected settings (especially rounding).\n",
    "deployed_specs = getattr(llama_mod, \"EXERCISE1_MX_SPECS\", None)\n",
    "if deployed_specs is not None:\n",
    "    try:\n",
    "        deployed_round = deployed_specs.get(\"round\") if hasattr(deployed_specs, \"get\") else deployed_specs[\"round\"]\n",
    "    except Exception:\n",
    "        deployed_round = \"<unknown>\"\n",
    "    print(f\"Deployed MX rounding mode: {deployed_round}\")\n",
    "\n",
    "def _trial2_semantics_present(src):\n",
    "    \"\"\"Returns (ok, reason). ok=True means bias and mx_specs are passed into the MX linear call.\"\"\"\n",
    "    try:\n",
    "        tree = ast.parse(src)\n",
    "    except SyntaxError as e:\n",
    "        return False, f\"Unable to parse deployed modeling_llama.py: {e}\"\n",
    "\n",
    "    apply_fn = None\n",
    "    for node in tree.body:\n",
    "        if isinstance(node, ast.FunctionDef) and node.name == \"apply_mx_linear\":\n",
    "            apply_fn = node\n",
    "            break\n",
    "    if apply_fn is None:\n",
    "        return False, \"apply_mx_linear not found\"\n",
    "\n",
    "    # Guard against Trial-1-style manual bias add after MX op.\n",
    "    for node in ast.walk(apply_fn):\n",
    "        if isinstance(node, ast.BinOp) and isinstance(node.op, ast.Add):\n",
    "            left_is_bias = isinstance(node.left, ast.Name) and node.left.id == \"bias\"\n",
    "            right_is_bias = isinstance(node.right, ast.Name) and node.right.id == \"bias\"\n",
    "            if left_is_bias or right_is_bias:\n",
    "                return False, \"Found a manual '+ bias' in apply_mx_linear (likely Trial 1)\"\n",
    "\n",
    "    # Accept both patterns used across iterations/versions:\n",
    "    # - mx.linear(..., bias=bias, mx_specs=mx_specs)\n",
    "    # - mx_linear.linear(..., bias=bias, mx_specs=mx_specs)\n",
    "    # - mx_fn(..., bias=bias, mx_specs=mx_specs) where mx_fn = getattr(mx_linear, 'linear', mx_linear)\n",
    "    def call_passes_bias_and_specs(call):\n",
    "        bias_ok = False\n",
    "        specs_ok = False\n",
    "\n",
    "        # Positional forms: (..., bias, mx_specs)\n",
    "        if len(call.args) >= 3 and isinstance(call.args[2], ast.Name) and call.args[2].id == \"bias\":\n",
    "            bias_ok = True\n",
    "        if len(call.args) >= 4 and isinstance(call.args[3], ast.Name) and call.args[3].id == \"mx_specs\":\n",
    "            specs_ok = True\n",
    "\n",
    "        # Keyword forms: bias=bias, mx_specs=mx_specs\n",
    "        for kw in call.keywords or []:\n",
    "            if kw.arg == \"bias\" and isinstance(kw.value, ast.Name) and kw.value.id == \"bias\":\n",
    "                bias_ok = True\n",
    "            if kw.arg in {\"mx_specs\", \"specs\"} and isinstance(kw.value, ast.Name) and kw.value.id == \"mx_specs\":\n",
    "                specs_ok = True\n",
    "\n",
    "        return bias_ok and specs_ok\n",
    "\n",
    "    for node in ast.walk(apply_fn):\n",
    "        if isinstance(node, ast.Call) and call_passes_bias_and_specs(node):\n",
    "            return True, \"\"\n",
    "\n",
    "    return False, \"No call found in apply_mx_linear that passes bias and mx_specs\"\n",
    "\n",
    "trial2_ok, trial2_reason = _trial2_semantics_present(deployed_src)\n",
    "print(f\"Trial 2 semantics detected: {trial2_ok}\")\n",
    "if not trial2_ok:\n",
    "    print(f\"Trial 2 semantics check detail: {trial2_reason}\")\n",
    "\n",
    "# 4) Smoke import of model classes\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM, LlamaMLP, LlamaAttention  # noqa: F401\n",
    "\n",
    "mlp_forward = inspect.getsource(LlamaMLP.forward)\n",
    "if \"apply_mx_linear\" not in mlp_forward:\n",
    "    print(\"Warning: apply_mx_linear not detected in LlamaMLP.forward source.\")\n",
    "else:\n",
    "    print(\"MX integration detected in LlamaMLP.forward.\")\n",
    "\n",
    "print(\"MX integration verification complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb0d8f1",
   "metadata": {
    "id": "7bb0d8f1"
   },
   "source": [
    "## Step 7: Quick Test (10% Dataset)\n",
    "\n",
    "Run a quick test to confirm the deployed code is the Trial 2 version and that `lm_eval` executes successfully.\n",
    "\n",
    "Estimated time: 1-2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2ce0f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying Trial 2 deployment in: /usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\n",
      "Trial 2 deployment verified\n",
      "\n",
      "+ lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32 --limit 0.1\n",
      "  env overrides: {'USE_MX_QUANTIZATION': '1', 'MX_W_ELEM_FORMAT': 'fp4_e2m1', 'MX_A_ELEM_FORMAT': 'fp6_e2m3', 'MX_BLOCK_SIZE': '32', 'MX_SCALE_BITS': '8', 'MX_SHARED_EXP_METHOD': 'max', 'MX_ROUND': 'even', 'MX_CUSTOM_CUDA': '1'}\n",
      "|lambada_openai|      1|none  |     0|acc       |↑  |0.5000|±  |0.0220|\n",
      "|              |       |none  |     0|perplexity|↓  |9.7963|±  |0.8769|\n"
     ]
    }
   ],
   "source": [
    "# Verify Trial 2 semantics are present in the deployed transformers file, then run a 10% eval.\n",
    "import ast\n",
    "import os\n",
    "import subprocess\n",
    "import transformers\n",
    "\n",
    "llama_file = transformers.models.llama.modeling_llama.__file__\n",
    "print(f\"Verifying Trial 2 deployment in: {llama_file}\")\n",
    "\n",
    "with open(llama_file, \"r\") as f:\n",
    "    deployed_src = f.read()\n",
    "\n",
    "def _trial2_semantics_present(src):\n",
    "    \"\"\"Returns (ok, reason). ok=True means bias and mx_specs are passed into the MX linear call.\"\"\"\n",
    "    try:\n",
    "        tree = ast.parse(src)\n",
    "    except SyntaxError as e:\n",
    "        return False, f\"Unable to parse deployed modeling_llama.py: {e}\"\n",
    "\n",
    "    apply_fn = None\n",
    "    for node in tree.body:\n",
    "        if isinstance(node, ast.FunctionDef) and node.name == \"apply_mx_linear\":\n",
    "            apply_fn = node\n",
    "            break\n",
    "    if apply_fn is None:\n",
    "        return False, \"apply_mx_linear not found\"\n",
    "\n",
    "    # Guard against Trial-1-style manual bias add after MX op.\n",
    "    for node in ast.walk(apply_fn):\n",
    "        if isinstance(node, ast.BinOp) and isinstance(node.op, ast.Add):\n",
    "            left_is_bias = isinstance(node.left, ast.Name) and node.left.id == \"bias\"\n",
    "            right_is_bias = isinstance(node.right, ast.Name) and node.right.id == \"bias\"\n",
    "            if left_is_bias or right_is_bias:\n",
    "                return False, \"Found a manual '+ bias' in apply_mx_linear (likely Trial 1)\"\n",
    "\n",
    "    def call_passes_bias_and_specs(call):\n",
    "        bias_ok = False\n",
    "        specs_ok = False\n",
    "\n",
    "        if len(call.args) >= 3 and isinstance(call.args[2], ast.Name) and call.args[2].id == \"bias\":\n",
    "            bias_ok = True\n",
    "        if len(call.args) >= 4 and isinstance(call.args[3], ast.Name) and call.args[3].id == \"mx_specs\":\n",
    "            specs_ok = True\n",
    "\n",
    "        for kw in call.keywords or []:\n",
    "            if kw.arg == \"bias\" and isinstance(kw.value, ast.Name) and kw.value.id == \"bias\":\n",
    "                bias_ok = True\n",
    "            if kw.arg in {\"mx_specs\", \"specs\"} and isinstance(kw.value, ast.Name) and kw.value.id == \"mx_specs\":\n",
    "                specs_ok = True\n",
    "\n",
    "        return bias_ok and specs_ok\n",
    "\n",
    "    for node in ast.walk(apply_fn):\n",
    "        if isinstance(node, ast.Call) and call_passes_bias_and_specs(node):\n",
    "            return True, \"\"\n",
    "\n",
    "    return False, \"No call found in apply_mx_linear that passes bias and mx_specs\"\n",
    "\n",
    "trial2_ok, trial2_reason = _trial2_semantics_present(deployed_src)\n",
    "if not trial2_ok:\n",
    "    raise RuntimeError(\n",
    "        \"Unable to confirm Trial 2 semantics in the deployed transformers file. \"\n",
    "        f\"Detail: {trial2_reason}. Re-run Step 4 (deploy) and restart the runtime, then try again.\"\n",
    "    )\n",
    "\n",
    "print(\"Trial 2 deployment verified\")\n",
    "\n",
    "# ----------------------\n",
    "# Quick eval runner that captures output and prints the final metrics table row.\n",
    "# ----------------------\n",
    "def run_lm_eval(env_overrides: dict, limit: float | None):\n",
    "    env = os.environ.copy()\n",
    "    env.update({k: str(v) for k, v in env_overrides.items()})\n",
    "    cmd = [\n",
    "        \"lm_eval\",\n",
    "        \"--model\", \"hf\",\n",
    "        \"--model_args\", \"pretrained=meta-llama/Llama-3.2-1B\",\n",
    "        \"--tasks\", \"lambada_openai\",\n",
    "        \"--device\", \"cuda\",\n",
    "        \"--batch_size\", \"32\",\n",
    "    ]\n",
    "    if limit is not None:\n",
    "        cmd += [\"--limit\", str(limit)]\n",
    "    print(\"\\n+\", \" \".join(cmd))\n",
    "    if env_overrides:\n",
    "        print(\"  env overrides:\", {k: env[k] for k in env_overrides})\n",
    "    out = subprocess.check_output(cmd, env=env, text=True, stderr=subprocess.STDOUT)\n",
    "    # Print the result row(s) for lambada_openai and perplexity for easy copy/paste\n",
    "    lines = out.splitlines()\n",
    "    for line in lines:\n",
    "        if \"|lambada_openai|\" in line or \"|              |\" in line and \"perplexity\" in line:\n",
    "            print(line)\n",
    "    return out\n",
    "\n",
    "# Default MX config (matches Exercise 1 target)\n",
    "mx_env = {\n",
    "    \"USE_MX_QUANTIZATION\": \"1\",\n",
    "    \"MX_W_ELEM_FORMAT\": \"fp4_e2m1\",\n",
    "    \"MX_A_ELEM_FORMAT\": \"fp6_e2m3\",\n",
    "    \"MX_BLOCK_SIZE\": \"32\",\n",
    "    \"MX_SCALE_BITS\": \"8\",\n",
    "    \"MX_SHARED_EXP_METHOD\": \"max\",\n",
    "    \"MX_ROUND\": \"even\",\n",
    "    \"MX_CUSTOM_CUDA\": \"1\",\n",
    "}\n",
    "\n",
    "_ = run_lm_eval(mx_env, limit=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2dbe94",
   "metadata": {
    "id": "ce2dbe94"
   },
   "source": [
    "## Step 8: Full Evaluation (Exercise 1)\n",
    "\n",
    "Run the complete evaluation with the MX-quantized model.\n",
    "\n",
    "Estimated time: 10-15 minutes\n",
    "Baseline: 62.10% accuracy\n",
    "Target: > 60% accuracy (< 2% degradation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5df3b4fa",
   "metadata": {
    "id": "5df3b4fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+ lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32\n",
      "  env overrides: {'USE_MX_QUANTIZATION': '0'}\n",
      "Parsed metrics: {'acc': 0.621, 'acc_stderr': 0.0068, 'perplexity': None, 'perplexity_stderr': None}\n",
      "\n",
      "+ lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32\n",
      "  env overrides: {'USE_MX_QUANTIZATION': '1', 'MX_W_ELEM_FORMAT': 'fp4_e2m1', 'MX_A_ELEM_FORMAT': 'fp6_e2m3', 'MX_BLOCK_SIZE': '32', 'MX_SCALE_BITS': '8', 'MX_SHARED_EXP_METHOD': 'max', 'MX_ROUND': 'even', 'MX_CUSTOM_CUDA': '1'}\n",
      "Parsed metrics: {'acc': 0.5067, 'acc_stderr': 0.007, 'perplexity': None, 'perplexity_stderr': None}\n",
      "\n",
      "+ lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32\n",
      "  env overrides: {'USE_MX_QUANTIZATION': '1', 'MX_W_ELEM_FORMAT': 'fp4_e2m1', 'MX_A_ELEM_FORMAT': 'fp6_e2m3', 'MX_BLOCK_SIZE': '32', 'MX_SCALE_BITS': '8', 'MX_SHARED_EXP_METHOD': 'max', 'MX_ROUND': 'even', 'MX_CUSTOM_CUDA': '0'}\n",
      "Parsed metrics: {'acc': 0.5084, 'acc_stderr': 0.007, 'perplexity': None, 'perplexity_stderr': None}\n",
      "\n",
      "+ lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32\n",
      "  env overrides: {'USE_MX_QUANTIZATION': '1', 'MX_W_ELEM_FORMAT': 'fp4_e2m1', 'MX_A_ELEM_FORMAT': 'None', 'MX_BLOCK_SIZE': '32', 'MX_SCALE_BITS': '8', 'MX_SHARED_EXP_METHOD': 'max', 'MX_ROUND': 'even', 'MX_CUSTOM_CUDA': '1'}\n",
      "Parsed metrics: {'acc': 0.5176, 'acc_stderr': 0.007, 'perplexity': None, 'perplexity_stderr': None}\n",
      "\n",
      "Summary (acc %):\n",
      "  baseline (MX off): 62.1\n",
      "  MX full: 50.67\n",
      "  MX full (custom_cuda=0): 50.839999999999996\n",
      "  MX weights-only: 51.76\n"
     ]
    }
   ],
   "source": [
    "# Full evaluation with MX quantization (with diagnostics and simple parsing).\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "\n",
    "def parse_lm_eval_metrics(text: str) -> dict:\n",
    "    # Extract the main acc row and perplexity row from the markdown table\n",
    "    acc = None\n",
    "    acc_stderr = None\n",
    "    ppl = None\n",
    "    ppl_stderr = None\n",
    "    for line in text.splitlines():\n",
    "        if \"|lambada_openai|\" in line and \"|acc\" in line:\n",
    "            # ... |acc|↑|0.5102|±|0.0070|\n",
    "            parts = [p.strip() for p in line.split(\"|\") if p.strip()]\n",
    "            # expect: lambada_openai, 1, none, 0, acc, ↑, value, ±, stderr\n",
    "            try:\n",
    "                acc = float(parts[6])\n",
    "                acc_stderr = float(parts[8])\n",
    "            except Exception:\n",
    "                pass\n",
    "        if \"perplexity\" in line and \"|              |\" in line:\n",
    "            parts = [p.strip() for p in line.split(\"|\") if p.strip()]\n",
    "            try:\n",
    "                ppl = float(parts[5])\n",
    "                ppl_stderr = float(parts[7])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return {\"acc\": acc, \"acc_stderr\": acc_stderr, \"perplexity\": ppl, \"perplexity_stderr\": ppl_stderr}\n",
    "\n",
    "def run_lm_eval(env_overrides: dict, limit: float | None):\n",
    "    env = os.environ.copy()\n",
    "    env.update({k: str(v) for k, v in env_overrides.items()})\n",
    "    cmd = [\n",
    "        \"lm_eval\",\n",
    "        \"--model\", \"hf\",\n",
    "        \"--model_args\", \"pretrained=meta-llama/Llama-3.2-1B\",\n",
    "        \"--tasks\", \"lambada_openai\",\n",
    "        \"--device\", \"cuda\",\n",
    "        \"--batch_size\", \"32\",\n",
    "    ]\n",
    "    if limit is not None:\n",
    "        cmd += [\"--limit\", str(limit)]\n",
    "    print(\"\\n+\", \" \".join(cmd))\n",
    "    if env_overrides:\n",
    "        print(\"  env overrides:\", {k: env[k] for k in env_overrides})\n",
    "    out = subprocess.check_output(cmd, env=env, text=True, stderr=subprocess.STDOUT)\n",
    "    metrics = parse_lm_eval_metrics(out)\n",
    "    print(\"Parsed metrics:\", metrics)\n",
    "    return out, metrics\n",
    "\n",
    "# 1) Runtime baseline (MX disabled)\n",
    "baseline_out, baseline_metrics = run_lm_eval({\"USE_MX_QUANTIZATION\": \"0\"}, limit=None)\n",
    "\n",
    "# 2) MX full (target config)\n",
    "mx_full_env = {\n",
    "    \"USE_MX_QUANTIZATION\": \"1\",\n",
    "    \"MX_W_ELEM_FORMAT\": \"fp4_e2m1\",\n",
    "    \"MX_A_ELEM_FORMAT\": \"fp6_e2m3\",\n",
    "    \"MX_BLOCK_SIZE\": \"32\",\n",
    "    \"MX_SCALE_BITS\": \"8\",\n",
    "    \"MX_SHARED_EXP_METHOD\": \"max\",\n",
    "    \"MX_ROUND\": \"even\",\n",
    "    \"MX_CUSTOM_CUDA\": \"1\",\n",
    "}\n",
    "mx_out, mx_metrics = run_lm_eval(mx_full_env, limit=None)\n",
    "\n",
    "# 3) MX full but custom CUDA disabled (diagnostic)\n",
    "mx_nocuda_out, mx_nocuda_metrics = run_lm_eval({**mx_full_env, \"MX_CUSTOM_CUDA\": \"0\"}, limit=None)\n",
    "\n",
    "# 4) MX weights-only (diagnostic)\n",
    "mx_wonly_out, mx_wonly_metrics = run_lm_eval({**mx_full_env, \"MX_A_ELEM_FORMAT\": \"None\"}, limit=None)\n",
    "\n",
    "# Persist for later cells\n",
    "baseline_acc = None if baseline_metrics[\"acc\"] is None else baseline_metrics[\"acc\"] * 100\n",
    "exercise1_acc = None if mx_metrics[\"acc\"] is None else mx_metrics[\"acc\"] * 100\n",
    "exercise1_acc_nocuda = None if mx_nocuda_metrics[\"acc\"] is None else mx_nocuda_metrics[\"acc\"] * 100\n",
    "exercise1_acc_wonly = None if mx_wonly_metrics[\"acc\"] is None else mx_wonly_metrics[\"acc\"] * 100\n",
    "print(\"\\nSummary (acc %):\")\n",
    "print(\"  baseline (MX off):\", baseline_acc)\n",
    "print(\"  MX full:\", exercise1_acc)\n",
    "print(\"  MX full (custom_cuda=0):\", exercise1_acc_nocuda)\n",
    "print(\"  MX weights-only:\", exercise1_acc_wonly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3f0c22",
   "metadata": {
    "id": "ae3f0c22"
   },
   "source": [
    "## Step 9: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fa235f3",
   "metadata": {
    "id": "1fa235f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: /content/msr-intern-project/Exercise1/results/exercise1_results.txt\n"
     ]
    }
   ],
   "source": [
    "# Save Exercise 1 results (auto-filled if Step 8 ran)\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "baseline_acc_doc = 62.10\n",
    "baseline_acc_runtime = globals().get(\"baseline_acc\", None)\n",
    "baseline_acc = baseline_acc_runtime if baseline_acc_runtime is not None else baseline_acc_doc\n",
    "\n",
    "exercise1_acc = globals().get(\"exercise1_acc\", None)\n",
    "exercise1_acc_nocuda = globals().get(\"exercise1_acc_nocuda\", None)\n",
    "exercise1_acc_wonly = globals().get(\"exercise1_acc_wonly\", None)\n",
    "\n",
    "def _fmt(v):\n",
    "    return \"<missing>\" if v is None else f\"{v:.2f}%\"\n",
    "\n",
    "results_content = f\"\"\"Exercise 1 Evaluation Results\n",
    "==================================================\n",
    "Timestamp: {timestamp}\n",
    "Model: meta-llama/Llama-3.2-1B\n",
    "Task: lambada_openai\n",
    "Device: CUDA\n",
    "Batch Size: 32\n",
    "\n",
    "MX Quantization Configuration (default):\n",
    "- Weight Format: fp4_e2m1 (4-bit)\n",
    "- Activation Format: fp6_e2m3 (6-bit)\n",
    "- Block Size: 32\n",
    "- Scale Bits: 8 (E8M0)\n",
    "- Shared Exp Method: max\n",
    "- Rounding: even\n",
    "- CUDA Backend (requested): Enabled\n",
    "\n",
    "Baseline Results:\n",
    "- Accuracy (runtime, MX off): {_fmt(baseline_acc_runtime)}\n",
    "- Accuracy (doc): {baseline_acc_doc:.2f}%\n",
    "- Using baseline: {_fmt(baseline_acc)}\n",
    "\n",
    "Exercise 1 Results:\n",
    "- MX full accuracy: {_fmt(exercise1_acc)}\n",
    "- MX full accuracy (custom_cuda=0): {_fmt(exercise1_acc_nocuda)}\n",
    "- MX weights-only accuracy (a=None): {_fmt(exercise1_acc_wonly)}\n",
    "\n",
    "Notes:\n",
    "- MX quantization applied to Q/K/V/O and gate/up/down via apply_mx_linear.\n",
    "- This file is generated by the notebook; keep HF tokens out of git.\n",
    "\"\"\"\n",
    "\n",
    "repo_root = \"/content/msr-intern-project\"\n",
    "base_dir = repo_root if os.path.isdir(repo_root) else os.getcwd()\n",
    "\n",
    "# Canonical repo location for committed artifacts\n",
    "results_dir = os.path.join(base_dir, \"results\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "results_path = os.path.join(results_dir, \"exercise1_results.txt\")\n",
    "with open(results_path, \"w\") as f:\n",
    "    f.write(results_content)\n",
    "\n",
    "print(f\"Results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf4df52",
   "metadata": {
    "id": "7bf4df52"
   },
   "source": [
    "## Step 10: Analysis & Comparison\n",
    "\n",
    "Compare Exercise 1 results with baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a28967f",
   "metadata": {
    "id": "5a28967f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXERCISE 1 RESULTS ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Baseline Accuracy (runtime): 62.10%\n",
      "Baseline Accuracy (doc):     62.10%\n",
      "Using baseline:             62.10%\n",
      "\n",
      "MX full (fp4 weights + fp6 activations): 50.67%\n",
      "MX full (custom_cuda=0):              50.84%\n",
      "MX weights-only (fp4, a=None):        51.76%\n",
      "\n",
      "Change vs baseline: -11.43% (-18.41%)\n",
      "Result: exceeds 2% degradation threshold\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Comparison analysis (auto-filled from Step 8 if available)\n",
    "print(\"=\" * 70)\n",
    "print(\"EXERCISE 1 RESULTS ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def _fmt(v):\n",
    "    return \"<missing>\" if v is None else f\"{v:.2f}%\"\n",
    "\n",
    "# Prefer the runtime-measured baseline if present; fall back to the doc baseline.\n",
    "baseline_acc_doc = 62.10\n",
    "baseline_acc_runtime = globals().get(\"baseline_acc\", None)\n",
    "baseline_acc = baseline_acc_runtime if baseline_acc_runtime is not None else baseline_acc_doc\n",
    "\n",
    "exercise1_acc = globals().get(\"exercise1_acc\", None)\n",
    "exercise1_acc_nocuda = globals().get(\"exercise1_acc_nocuda\", None)\n",
    "exercise1_acc_wonly = globals().get(\"exercise1_acc_wonly\", None)\n",
    "\n",
    "print(f\"\\nBaseline Accuracy (runtime): {_fmt(baseline_acc_runtime)}\")\n",
    "print(f\"Baseline Accuracy (doc):     {baseline_acc_doc:.2f}%\")\n",
    "print(f\"Using baseline:             {_fmt(baseline_acc)}\")\n",
    "\n",
    "print(f\"\\nMX full (fp4 weights + fp6 activations): {_fmt(exercise1_acc)}\")\n",
    "print(f\"MX full (custom_cuda=0):              {_fmt(exercise1_acc_nocuda)}\")\n",
    "print(f\"MX weights-only (fp4, a=None):        {_fmt(exercise1_acc_wonly)}\")\n",
    "\n",
    "if exercise1_acc is not None:\n",
    "    accuracy_change = exercise1_acc - baseline_acc\n",
    "    accuracy_change_pct = (accuracy_change / baseline_acc) * 100\n",
    "    print(f\"\\nChange vs baseline: {accuracy_change:+.2f}% ({accuracy_change_pct:+.2f}%)\")\n",
    "    if accuracy_change >= -2.0:\n",
    "        print(\"Result: within target (< 2% degradation)\")\n",
    "    else:\n",
    "        print(\"Result: exceeds 2% degradation threshold\")\n",
    "else:\n",
    "    print(\"\\nRun Step 8 first to populate metrics.\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ba5b3e",
   "metadata": {
    "id": "92ba5b3e"
   },
   "source": [
    "## Exercise 1 Wrap-up\n",
    "\n",
    "**Next steps**\n",
    "1. Record results in `results/exercise1_results.txt`\n",
    "2. Compare accuracy vs baseline (62.10%)\n",
    "3. Commit and push non-secret outputs (do not commit HF tokens)\n",
    "\n",
    "**Artifacts**\n",
    "- `Exercise1/modified_files/modeling_llama.py`\n",
    "- `Exercise1/mx_config_helper.py`\n",
    "- `results/exercise1_results.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872facb5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
