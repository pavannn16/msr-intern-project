{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "638e2d14",
   "metadata": {
    "id": "638e2d14"
   },
   "source": [
    "# Exercise 1: MX Quantization of Linear Layers\n",
    "\n",
    "## Llama-3.2-1B with mxfp4_e2m1 (weights) + mxfp6_e2m3 (activations)\n",
    "\n",
    "This notebook evaluates an MX-quantized Llama model on the `lambada_openai` task.\n",
    "\n",
    "**Exercise objectives**\n",
    "- Quantize all linear layers (Q, K, V, O, gate, up, down)\n",
    "- Use `mxfp4_e2m1` for weights (4-bit)\n",
    "- Use `mxfp6_e2m3` for activations (6-bit)\n",
    "- Compare accuracy vs baseline (62.10%)\n",
    "\n",
    "**Expected outcome**\n",
    "- Accuracy target: > 60% (< 2% degradation)\n",
    "\n",
    "**Author:** Pavan Chauhan  \n",
    "**Date:** January 30, 2026\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is designed to run top-to-bottom without manual restarts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270619e7",
   "metadata": {
    "id": "270619e7"
   },
   "source": [
    "## Step 1: Verify GPU Runtime\n",
    "\n",
    "Ensure GPU runtime is enabled (T4/A100/H100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4b91c23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4b91c23",
    "outputId": "bc29570a-093c-43a2-8a7a-9c19092881fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan 31 04:58:36 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   34C    P0             47W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488c0506",
   "metadata": {
    "id": "488c0506"
   },
   "source": [
    "## Step 2: Clone Project Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c04e39c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0c04e39c",
    "outputId": "bda364e2-fb74-4e2b-ff09-db999671afce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/msr-intern-project\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clone / update the project repository\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "repo_dir = \"/content/msr-intern-project\"\n",
    "if not os.path.exists(repo_dir):\n",
    "    subprocess.check_call([\"git\", \"clone\", \"https://github.com/pavannn16/msr-intern-project.git\", repo_dir])\n",
    "else:\n",
    "    subprocess.check_call([\"git\", \"-C\", repo_dir, \"fetch\", \"origin\", \"main\"])\n",
    "    subprocess.check_call([\"git\", \"-C\", repo_dir, \"reset\", \"--hard\", \"origin/main\"])\n",
    "\n",
    "%cd /content/msr-intern-project\n",
    "subprocess.check_call([\"git\", \"rev-parse\", \"--short\", \"HEAD\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c00750b",
   "metadata": {
    "id": "8c00750b"
   },
   "source": [
    "## Step 3: Install Dependencies\n",
    "\n",
    "Install `transformers`, `microxcaling` (MX), and `lm-eval`.\n",
    "\n",
    "Estimated time: 2-3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d67f3c57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d67f3c57",
    "outputId": "c65a55cc-2310-4d34-c793-11c6068269a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies...\n",
      "+ /usr/bin/python3 -m pip install -q transformers==4.57.6 lm_eval ninja\n",
      "torch=2.9.0+cu126, cuda=12.6, device_count=1\n",
      "microxcaling already exists\n",
      "+ /usr/bin/python3 -m pip install -q -e /content/microxcaling --no-deps\n",
      "MX import: OK\n",
      "All dependencies installed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"Installing dependencies...\")\n",
    "\n",
    "def pip_install(args):\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + args\n",
    "    print(\"+\", \" \".join(cmd))\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "# Core packages (do not force-install torch; Colab typically provides a working CUDA build)\n",
    "pip_install([\"transformers==4.57.6\", \"lm_eval\", \"ninja\"])\n",
    "\n",
    "# Confirm torch is available and report version\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"torch={torch.__version__}, cuda={torch.version.cuda}, device_count={torch.cuda.device_count()}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"PyTorch is not available in this runtime: {e}\")\n",
    "\n",
    "# Clone microxcaling (MX)\n",
    "mx_repo_dir = \"/content/microxcaling\"\n",
    "if not os.path.exists(mx_repo_dir):\n",
    "    subprocess.check_call([\"git\", \"clone\", \"-q\", \"https://github.com/microsoft/microxcaling.git\", mx_repo_dir])\n",
    "    print(\"microxcaling cloned\")\n",
    "else:\n",
    "    print(\"microxcaling already exists\")\n",
    "\n",
    "# Install microxcaling WITHOUT deps to avoid torchaudio pinning issues.\n",
    "# The Python package is commonly located under a subdirectory (e.g., /python).\n",
    "mx_python_root = os.path.join(mx_repo_dir, \"python\") if os.path.isdir(os.path.join(mx_repo_dir, \"python\")) else mx_repo_dir\n",
    "pip_install([\"-e\", mx_python_root, \"--no-deps\"])\n",
    "\n",
    "# Sanity check: MX import (fallback to sys.path if editable install layout differs)\n",
    "try:\n",
    "    import mx  # noqa: F401\n",
    "except ModuleNotFoundError:\n",
    "    if mx_python_root not in sys.path:\n",
    "        sys.path.insert(0, mx_python_root)\n",
    "    import mx  # noqa: F401\n",
    "\n",
    "print(\"MX import: OK\")\n",
    "print(\"All dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f3f3ef",
   "metadata": {
    "id": "64f3f3ef"
   },
   "source": [
    "## Step 4: Setup Exercise 1 Files\n",
    "\n",
    "Copy the complete MX-integrated modeling_llama.py to transformers package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47501700",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47501700",
    "outputId": "a11967fa-e715-414b-9138-e94be4d4f9e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Exercise 1 files...\n",
      "Transformers installed at: /usr/local/lib/python3.12/dist-packages/transformers\n",
      "Found MX-integrated file (703 lines)\n",
      "MX-integrated modeling_llama.py deployed\n",
      "MX quantization functions detected in deployed file\n",
      "\n",
      "Exercise 1 setup complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import transformers\n",
    "\n",
    "print(\"Setting up Exercise 1 files...\")\n",
    "\n",
    "transformers_path = os.path.dirname(transformers.__file__)\n",
    "print(f\"Transformers installed at: {transformers_path}\")\n",
    "\n",
    "modeling_llama_path = os.path.join(transformers_path, \"models\", \"llama\", \"modeling_llama.py\")\n",
    "backup_path = modeling_llama_path + \".backup\"\n",
    "\n",
    "mx_complete_file = \"/content/msr-intern-project/Exercise1/modified_files/modeling_llama.py\"\n",
    "if os.path.exists(mx_complete_file):\n",
    "    with open(mx_complete_file, \"r\") as f:\n",
    "        line_count = len(f.readlines())\n",
    "    print(f\"Found MX-integrated file ({line_count} lines)\")\n",
    "\n",
    "    if not os.path.exists(backup_path):\n",
    "        shutil.copy2(modeling_llama_path, backup_path)\n",
    "        print(\"Backup created\")\n",
    "\n",
    "    shutil.copy2(mx_complete_file, modeling_llama_path)\n",
    "    print(\"MX-integrated modeling_llama.py deployed\")\n",
    "\n",
    "    with open(modeling_llama_path, \"r\") as f:\n",
    "        content = f.read()\n",
    "    if \"apply_mx_linear\" in content:\n",
    "        print(\"MX quantization functions detected in deployed file\")\n",
    "    else:\n",
    "        print(\"Warning: MX functions not found in deployed file\")\n",
    "else:\n",
    "    print(f\"Warning: MX file not found at {mx_complete_file}\")\n",
    "    print(\"  Evaluation will use standard transformers (no MX quantization)\")\n",
    "\n",
    "print(\"\\nExercise 1 setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8839dd3a",
   "metadata": {
    "id": "8839dd3a"
   },
   "source": [
    "## Step 5: Set Hugging Face Token\n",
    "\n",
    "Set your Hugging Face token for model access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9b2fa9",
   "metadata": {
    "id": "af9b2fa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF token is set (value hidden)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Option A (recommended on Colab): use Secrets\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
    "\n",
    "# Option B: manually set the token in this runtime (do not commit real tokens)\n",
    "os.environ[\"HF_TOKEN\"] = \"YOUR_HF_TOKEN_HERE\"\n",
    "\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if not hf_token or hf_token == \"YOUR_HF_TOKEN_HERE\":\n",
    "    raise RuntimeError(\n",
    "        \"HF_TOKEN is not set. Set it via Colab Secrets (recommended) or set the token in this cell for runtime-only use (do not commit real tokens).\"\n",
    "    )\n",
    "\n",
    "print(\"HF token is set (value hidden)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56d8ed7",
   "metadata": {
    "id": "b56d8ed7"
   },
   "source": [
    "## Step 6: Verify MX Integration\n",
    "\n",
    "Test that MX library and modified model load correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2002d63b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "2002d63b",
    "outputId": "ac3da440-bdf8-47a1-9595-55ef4a90d62d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying MX integration...\n",
      "MX library import: OK\n",
      "MX Quantization Configuration:\n",
      "==================================================\n",
      "Weights: fp4_e2m1 (4-bit)\n",
      "Activations: fp6_e2m3 (6-bit)\n",
      "Scale Bits: 8 (E8M0)\n",
      "Block Size: 32\n",
      "CUDA Backend: Enabled\n",
      "Rounding: even\n",
      "Backward Quantization: Disabled\n",
      "==================================================\n",
      "Transformers llama module: /usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\n",
      "Deployed MX rounding mode: even\n",
      "Trial 2 semantics detected: False\n",
      "Trial 2 semantics check detail: No mx.linear/mx_linear.linear call found in apply_mx_linear\n",
      "MX integration detected in LlamaMLP.forward.\n",
      "MX integration verification complete\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import inspect\n",
    "import sys\n",
    "\n",
    "print(\"Verifying MX integration...\")\n",
    "\n",
    "exercise1_dir = \"/content/msr-intern-project/Exercise1\"\n",
    "if exercise1_dir not in sys.path:\n",
    "    sys.path.insert(0, exercise1_dir)\n",
    "\n",
    "# 1) MX import\n",
    "try:\n",
    "    from mx import linear as mx_linear  # noqa: F401\n",
    "    from mx.specs import MxSpecs  # noqa: F401\n",
    "    print(\"MX library import: OK\")\n",
    "except ImportError as e:\n",
    "    raise ImportError(\n",
    "        f\"MX library import failed: {e}. Ensure Step 3 installed microxcaling (pip install -e /content/microxcaling).\"\n",
    "    )\n",
    "\n",
    "# 2) Local helper import\n",
    "try:\n",
    "    from mx_config_helper import create_mx_specs_exercise1, print_mx_specs_summary\n",
    "    mx_specs = create_mx_specs_exercise1()\n",
    "    print_mx_specs_summary(mx_specs)\n",
    "except ImportError as e:\n",
    "    raise ImportError(\n",
    "        f\"Failed to import Exercise 1 helper(s): {e}. Ensure the repo is cloned and Step 2 ran successfully.\"\n",
    "    )\n",
    "\n",
    "# 3) Confirm the deployed transformers Llama file is the MX-integrated one\n",
    "import transformers\n",
    "\n",
    "llama_mod = transformers.models.llama.modeling_llama\n",
    "llama_file = getattr(llama_mod, \"__file__\", \"<unknown>\")\n",
    "print(f\"Transformers llama module: {llama_file}\")\n",
    "\n",
    "with open(llama_file, \"r\") as f:\n",
    "    deployed_src = f.read()\n",
    "\n",
    "if \"apply_mx_linear\" not in deployed_src:\n",
    "    raise RuntimeError(\n",
    "        \"Deployed transformers modeling_llama.py does not appear to include MX integration. Re-run Step 4 (deploy).\"\n",
    "    )\n",
    "\n",
    "# Extra: confirm the deployed MX specs match expected settings (especially rounding).\n",
    "deployed_specs = getattr(llama_mod, \"EXERCISE1_MX_SPECS\", None)\n",
    "if deployed_specs is not None:\n",
    "    try:\n",
    "        deployed_round = deployed_specs.get(\"round\") if hasattr(deployed_specs, \"get\") else deployed_specs[\"round\"]\n",
    "    except Exception:\n",
    "        deployed_round = \"<unknown>\"\n",
    "    print(f\"Deployed MX rounding mode: {deployed_round}\")\n",
    "\n",
    "def _trial2_semantics_present(src):\n",
    "    \"\"\"Returns (ok, reason). ok=True means bias and mx_specs are passed into the MX linear call.\"\"\"\n",
    "    try:\n",
    "        tree = ast.parse(src)\n",
    "    except SyntaxError as e:\n",
    "        return False, f\"Unable to parse deployed modeling_llama.py: {e}\"\n",
    "\n",
    "    apply_fn = None\n",
    "    for node in tree.body:\n",
    "        if isinstance(node, ast.FunctionDef) and node.name == \"apply_mx_linear\":\n",
    "            apply_fn = node\n",
    "            break\n",
    "    if apply_fn is None:\n",
    "        return False, \"apply_mx_linear not found\"\n",
    "\n",
    "    # Guard against Trial-1-style manual bias add after MX op.\n",
    "    for node in ast.walk(apply_fn):\n",
    "        if isinstance(node, ast.BinOp) and isinstance(node.op, ast.Add):\n",
    "            left_is_bias = isinstance(node.left, ast.Name) and node.left.id == \"bias\"\n",
    "            right_is_bias = isinstance(node.right, ast.Name) and node.right.id == \"bias\"\n",
    "            if left_is_bias or right_is_bias:\n",
    "                return False, \"Found a manual '+ bias' in apply_mx_linear (likely Trial 1)\"\n",
    "\n",
    "    # Accept both patterns used across iterations/versions:\n",
    "    # - mx.linear(..., bias=bias, mx_specs=mx_specs)\n",
    "    # - mx_linear.linear(..., bias=bias, mx_specs=mx_specs)\n",
    "    # - mx_fn(..., bias=bias, mx_specs=mx_specs) where mx_fn = getattr(mx_linear, 'linear', mx_linear)\n",
    "    def call_passes_bias_and_specs(call):\n",
    "        bias_ok = False\n",
    "        specs_ok = False\n",
    "\n",
    "        # Positional forms: (..., bias, mx_specs)\n",
    "        if len(call.args) >= 3 and isinstance(call.args[2], ast.Name) and call.args[2].id == \"bias\":\n",
    "            bias_ok = True\n",
    "        if len(call.args) >= 4 and isinstance(call.args[3], ast.Name) and call.args[3].id == \"mx_specs\":\n",
    "            specs_ok = True\n",
    "\n",
    "        # Keyword forms: bias=bias, mx_specs=mx_specs\n",
    "        for kw in call.keywords or []:\n",
    "            if kw.arg == \"bias\" and isinstance(kw.value, ast.Name) and kw.value.id == \"bias\":\n",
    "                bias_ok = True\n",
    "            if kw.arg in {\"mx_specs\", \"specs\"} and isinstance(kw.value, ast.Name) and kw.value.id == \"mx_specs\":\n",
    "                specs_ok = True\n",
    "\n",
    "        return bias_ok and specs_ok\n",
    "\n",
    "    for node in ast.walk(apply_fn):\n",
    "        if isinstance(node, ast.Call) and call_passes_bias_and_specs(node):\n",
    "            return True, \"\"\n",
    "\n",
    "    return False, \"No call found in apply_mx_linear that passes bias and mx_specs\"\n",
    "\n",
    "trial2_ok, trial2_reason = _trial2_semantics_present(deployed_src)\n",
    "print(f\"Trial 2 semantics detected: {trial2_ok}\")\n",
    "if not trial2_ok:\n",
    "    print(f\"Trial 2 semantics check detail: {trial2_reason}\")\n",
    "\n",
    "# 4) Smoke import of model classes\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM, LlamaMLP, LlamaAttention  # noqa: F401\n",
    "\n",
    "mlp_forward = inspect.getsource(LlamaMLP.forward)\n",
    "if \"apply_mx_linear\" not in mlp_forward:\n",
    "    print(\"Warning: apply_mx_linear not detected in LlamaMLP.forward source.\")\n",
    "else:\n",
    "    print(\"MX integration detected in LlamaMLP.forward.\")\n",
    "\n",
    "print(\"MX integration verification complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb0d8f1",
   "metadata": {
    "id": "7bb0d8f1"
   },
   "source": [
    "## Step 7: Quick Test (10% Dataset)\n",
    "\n",
    "Run a quick test to confirm the deployed code is the Trial 2 version and that `lm_eval` executes successfully.\n",
    "\n",
    "Estimated time: 1-2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ce0f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying Trial 2 deployment in: /usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unable to confirm Trial 2 semantics in the deployed transformers file. Detail: No mx.linear/mx_linear.linear call found in apply_mx_linear. Re-run Step 4 (deploy) and restart the runtime, then try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3499124942.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mtrial2_ok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial2_reason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_trial2_semantics_present\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeployed_src\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrial2_ok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     raise RuntimeError(\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;34m\"Unable to confirm Trial 2 semantics in the deployed transformers file. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;34mf\"Detail: {trial2_reason}. Re-run Step 4 (deploy) and restart the runtime, then try again.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to confirm Trial 2 semantics in the deployed transformers file. Detail: No mx.linear/mx_linear.linear call found in apply_mx_linear. Re-run Step 4 (deploy) and restart the runtime, then try again."
     ]
    }
   ],
   "source": [
    "# Verify Trial 2 semantics are present in the deployed transformers file, then run a 10% eval.\n",
    "import ast\n",
    "import transformers\n",
    "\n",
    "llama_file = transformers.models.llama.modeling_llama.__file__\n",
    "print(f\"Verifying Trial 2 deployment in: {llama_file}\")\n",
    "\n",
    "with open(llama_file, \"r\") as f:\n",
    "    deployed_src = f.read()\n",
    "\n",
    "def _trial2_semantics_present(src):\n",
    "    \"\"\"Returns (ok, reason). ok=True means bias and mx_specs are passed into the MX linear call.\"\"\"\n",
    "    try:\n",
    "        tree = ast.parse(src)\n",
    "    except SyntaxError as e:\n",
    "        return False, f\"Unable to parse deployed modeling_llama.py: {e}\"\n",
    "\n",
    "    apply_fn = None\n",
    "    for node in tree.body:\n",
    "        if isinstance(node, ast.FunctionDef) and node.name == \"apply_mx_linear\":\n",
    "            apply_fn = node\n",
    "            break\n",
    "    if apply_fn is None:\n",
    "        return False, \"apply_mx_linear not found\"\n",
    "\n",
    "    # Guard against Trial-1-style manual bias add after MX op.\n",
    "    for node in ast.walk(apply_fn):\n",
    "        if isinstance(node, ast.BinOp) and isinstance(node.op, ast.Add):\n",
    "            left_is_bias = isinstance(node.left, ast.Name) and node.left.id == \"bias\"\n",
    "            right_is_bias = isinstance(node.right, ast.Name) and node.right.id == \"bias\"\n",
    "            if left_is_bias or right_is_bias:\n",
    "                return False, \"Found a manual '+ bias' in apply_mx_linear (likely Trial 1)\"\n",
    "\n",
    "    def call_passes_bias_and_specs(call):\n",
    "        bias_ok = False\n",
    "        specs_ok = False\n",
    "\n",
    "        if len(call.args) >= 3 and isinstance(call.args[2], ast.Name) and call.args[2].id == \"bias\":\n",
    "            bias_ok = True\n",
    "        if len(call.args) >= 4 and isinstance(call.args[3], ast.Name) and call.args[3].id == \"mx_specs\":\n",
    "            specs_ok = True\n",
    "\n",
    "        for kw in call.keywords or []:\n",
    "            if kw.arg == \"bias\" and isinstance(kw.value, ast.Name) and kw.value.id == \"bias\":\n",
    "                bias_ok = True\n",
    "            if kw.arg in {\"mx_specs\", \"specs\"} and isinstance(kw.value, ast.Name) and kw.value.id == \"mx_specs\":\n",
    "                specs_ok = True\n",
    "\n",
    "        return bias_ok and specs_ok\n",
    "\n",
    "    for node in ast.walk(apply_fn):\n",
    "        if isinstance(node, ast.Call) and call_passes_bias_and_specs(node):\n",
    "            return True, \"\"\n",
    "\n",
    "    return False, \"No call found in apply_mx_linear that passes bias and mx_specs\"\n",
    "\n",
    "trial2_ok, trial2_reason = _trial2_semantics_present(deployed_src)\n",
    "if not trial2_ok:\n",
    "    raise RuntimeError(\n",
    "        \"Unable to confirm Trial 2 semantics in the deployed transformers file. \"\n",
    "        f\"Detail: {trial2_reason}. Re-run Step 4 (deploy) and restart the runtime, then try again.\"\n",
    "    )\n",
    "\n",
    "print(\"Trial 2 deployment verified\")\n",
    "!lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32 --limit 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2dbe94",
   "metadata": {
    "id": "ce2dbe94"
   },
   "source": [
    "## Step 8: Full Evaluation (Exercise 1)\n",
    "\n",
    "Run the complete evaluation with the MX-quantized model.\n",
    "\n",
    "Estimated time: 10-15 minutes\n",
    "Baseline: 62.10% accuracy\n",
    "Target: > 60% accuracy (< 2% degradation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df3b4fa",
   "metadata": {
    "id": "5df3b4fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-31:04:51:38 INFO     [_cli.run:376] Selected Tasks: ['lambada_openai']\n",
      "2026-01-31:04:51:40 INFO     [evaluator:211] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2026-01-31:04:51:40 INFO     [evaluator:236] Initializing hf model, with arguments: {'pretrained': 'meta-llama/Llama-3.2-1B'}\n",
      "2026-01-31:04:51:43 INFO     [models.huggingface:161] Using device 'cuda'\n",
      "2026-01-31:04:51:44 INFO     [models.huggingface:423] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}\n",
      "2026-01-31 04:51:45.670693: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1769835105.691169    1529 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1769835105.696334    1529 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1769835105.709381    1529 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769835105.709405    1529 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769835105.709408    1529 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769835105.709411    1529 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-31:04:51:55 INFO     [tasks:700] Selected tasks:\n",
      "2026-01-31:04:51:55 INFO     [tasks:691] Task: lambada_openai (lambada/lambada_openai.yaml)\n",
      "2026-01-31:04:51:55 INFO     [api.task:311] Building contexts for lambada_openai on rank 0...\n",
      " 60% 3076/5153 [00:07<00:05, 390.37it/s]Exception ignored in: <generator object tqdm.__iter__ at 0x7bdbea4a7d00>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tqdm/std.py\", line 1196, in __iter__\n",
      "    self.close()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tqdm/std.py\", line 1302, in close\n",
      "    self.display(pos=0)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tqdm/std.py\", line 1495, in display\n",
      "    self.sp(self.__str__() if msg is None else msg)\n",
      "            ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tqdm/std.py\", line 1151, in __str__\n",
      "    return self.format_meter(**self.format_dict)\n",
      "                               ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tqdm/std.py\", line 1446, in format_dict\n",
      "    @property\n",
      "\n",
      "KeyboardInterrupt: \n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/lm_eval\", line 8, in <module>\n",
      "    sys.exit(cli_evaluate())\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/__main__.py\", line 10, in cli_evaluate\n",
      "    parser.execute(args)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/_cli/harness.py\", line 60, in execute\n",
      "    args.func(args)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/_cli/run.py\", line 379, in _execute\n",
      "    results = simple_evaluate(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/utils.py\", line 498, in _wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/evaluator.py\", line 366, in simple_evaluate\n",
      "    results = evaluate(\n",
      "              ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/utils.py\", line 498, in _wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/evaluator.py\", line 534, in evaluate\n",
      "    task.build_all_requests(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/api/task.py\", line 349, in build_all_requests\n",
      "    inst = self.construct_requests(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/api/task.py\", line 1366, in construct_requests\n",
      "    arguments = (ctx, self.doc_to_target(doc))\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/api/task.py\", line 1248, in doc_to_target\n",
      "    target_string = utils.apply_template(doc_to_target, doc)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/lm_eval/utils.py\", line 591, in apply_template\n",
      "    rtemplate = env.from_string(template)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/jinja2/environment.py\", line 1111, in from_string\n",
      "    return cls.from_code(self, self.compile(source), gs, None)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/jinja2/environment.py\", line 764, in compile\n",
      "    source = self._generate(source, name, filename, defer_init=defer_init)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/jinja2/environment.py\", line 694, in _generate\n",
      "    return generate(  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/jinja2/compiler.py\", line 117, in generate\n",
      "    generator.visit(node)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/jinja2/visitor.py\", line 40, in visit\n",
      "    return f(node, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/jinja2/compiler.py\", line 854, in visit_Template\n",
      "    for block in node.find_all(nodes.Block):\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/jinja2/nodes.py\", line 204, in find_all\n",
      "    yield from child.find_all(node_type)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/jinja2/nodes.py\", line 204, in find_all\n",
      "    yield from child.find_all(node_type)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/jinja2/nodes.py\", line 204, in find_all\n",
      "    yield from child.find_all(node_type)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/jinja2/nodes.py\", line 195, in find_all\n",
      "    def find_all(\n",
      "\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Full evaluation with MX quantization\n",
    "!lm_eval --model hf --model_args pretrained=meta-llama/Llama-3.2-1B --tasks lambada_openai --device cuda --batch_size 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3f0c22",
   "metadata": {
    "id": "ae3f0c22"
   },
   "source": [
    "## Step 9: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa235f3",
   "metadata": {
    "id": "1fa235f3"
   },
   "outputs": [],
   "source": [
    "# Save Exercise 1 results\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "results_content = f\"\"\"Exercise 1 Evaluation Results\n",
    "==================================================\n",
    "Timestamp: {timestamp}\n",
    "Model: meta-llama/Llama-3.2-1B\n",
    "Task: lambada_openai\n",
    "Device: CUDA\n",
    "Batch Size: 32\n",
    "\n",
    "MX Quantization Configuration:\n",
    "- Weight Format: mxfp4_e2m1 (4-bit)\n",
    "- Activation Format: mxfp6_e2m3 (6-bit)\n",
    "- Block Size: 32\n",
    "- Scale Bits: 8 (E8M0)\n",
    "- CUDA Backend: Enabled\n",
    "\n",
    "Baseline Results (for comparison):\n",
    "- Accuracy: 62.10%\n",
    "- Runtime: ~22 seconds\n",
    "\n",
    "Exercise 1 Results:\n",
    "- Accuracy: [FILL FROM lm_eval OUTPUT]\n",
    "- Perplexity: [FILL FROM lm_eval OUTPUT]\n",
    "- Runtime: [FILL]\n",
    "- Accuracy Change: [CALCULATE vs baseline]\n",
    "\n",
    "Notes:\n",
    "- MX quantization applied to all linear layers\n",
    "- Q, K, V, O projections (attention)\n",
    "- gate, up, down projections (MLP)\n",
    "- Block-floating-point with shared exponent\n",
    "\n",
    "Status: [SUCCESS/FAILED]\n",
    "Comments: [Add observations here]\n",
    "\"\"\"\n",
    "\n",
    "# Write under the repo if present; otherwise fall back to the current working directory.\n",
    "repo_root = \"/content/msr-intern-project\"\n",
    "base_dir = repo_root if os.path.isdir(repo_root) else os.getcwd()\n",
    "\n",
    "results_dir = os.path.join(base_dir, \"Exercise1\", \"results\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "results_path = os.path.join(results_dir, \"exercise1_results.txt\")\n",
    "with open(results_path, \"w\") as f:\n",
    "    f.write(results_content)\n",
    "\n",
    "print(f\"Results template saved to: {results_path}\")\n",
    "print(\"Update the file with metrics from the evaluation output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf4df52",
   "metadata": {
    "id": "7bf4df52"
   },
   "source": [
    "## Step 10: Analysis & Comparison\n",
    "\n",
    "Compare Exercise 1 results with baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a28967f",
   "metadata": {
    "id": "5a28967f"
   },
   "outputs": [],
   "source": [
    "# Comparison analysis\n",
    "print(\"=\" * 70)\n",
    "print(\"EXERCISE 1 RESULTS ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "baseline_acc = 62.10\n",
    "exercise1_acc = 0.0  # TODO: Fill from your results\n",
    "\n",
    "if exercise1_acc > 0:\n",
    "    accuracy_change = exercise1_acc - baseline_acc\n",
    "    accuracy_change_pct = (accuracy_change / baseline_acc) * 100\n",
    "\n",
    "    print(f\"\\nBaseline Accuracy:   {baseline_acc:.2f}%\")\n",
    "    print(f\"Exercise 1 Accuracy: {exercise1_acc:.2f}%\")\n",
    "    print(f\"Change:              {accuracy_change:+.2f}% ({accuracy_change_pct:+.2f}%)\")\n",
    "    print()\n",
    "\n",
    "    if accuracy_change >= -2.0:\n",
    "        print(\"Result: within target (< 2% degradation)\")\n",
    "    else:\n",
    "        print(\"Result: exceeds 2% degradation threshold\")\n",
    "\n",
    "    print()\n",
    "    print(\"Memory savings (theoretical):\")\n",
    "    print(\"  - Weights: 75% reduction (FP32 -> mxfp4_e2m1)\")\n",
    "    print(\"  - Activations: 81% reduction (FP32 -> mxfp6_e2m3)\")\n",
    "else:\n",
    "    print(\"\\nSet exercise1_acc to your actual accuracy from Step 8 output, then re-run this cell.\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ba5b3e",
   "metadata": {
    "id": "92ba5b3e"
   },
   "source": [
    "## Exercise 1 Wrap-up\n",
    "\n",
    "**Next steps**\n",
    "1. Record results in `Exercise1/results/exercise1_results.txt`\n",
    "2. Compare accuracy vs baseline (62.10%)\n",
    "3. Commit and push non-secret outputs (do not commit HF tokens)\n",
    "\n",
    "**Artifacts**\n",
    "- `Exercise1/modified_files/modeling_llama.py`\n",
    "- `Exercise1/mx_config_helper.py`\n",
    "- `Exercise1/results/exercise1_results.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872facb5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
